%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Comp 550: Fall 2023\\Reading Assignment 2}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

%----------------------------------------------------------------------------------------
%	AUTHOR AFFILIATIONS
%----------------------------------------------------------------------------------------

\let\thefootnote\relax\footnotetext{* \textit{PhD Student, School of Computer Science, McGill University, Montreal, Canada}}

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Overview}

In the paper "Grammar as a foreign language" by \citet{vinyals2015grammar}, the authors employ an attention-enhanced sequence-to-sequence model for syntactic constituency parsing, attaining state-of-the-art results. Building on prior work, notably \citet{sutskever2014sequence}'s sequence-to-sequence model and \citet{bahdanau2014neural}'s attention model for long sequences, they tackled parsing as a sequence-to-sequence problem with linearized parse trees. Initial trials with Sutskever et al's model on standard datasets underperformed. However, using a larger artificially annotated dataset, they matched the BerkeleyParser's F1 score of 90.5. With Bahdanau et al's model, they achieved similar results without data augmentation. By focusing on high-confidence parse trees, they achieved an F1 score of 92.1 on section 23 of the the Wall Street Journal.

The authors matched state-of-the-art by creating a larger training set of 11M parsed sentences (250M tokens). They collected a range of well known publicly available curated linguistic datasets such as treebank and the OntoNotes corpus. This resulted in a total of 90K training sentences, so to reach 11M sentences they used their reimplementation of BerkeleyParser and a reimplementation of ZPar to process unlabelled sentences from news sources on the web and selected only those sentences for which both parsers produced the same tree and resampled to match the sentence length distribution of the WSJ training corpus. These efforts allowed the model to break the state-of-the-art, because their model had more, higher quality training data than the other approaches and also utilised a model with stronger inductive biases (attention mechanism).

The paper reports improvements on the state-of-the-art performance in syntactic constituency parsing, and they also report that the attention based model is significantly faster even with CPU-only inference. The LSTM+A model is capable of parsing over 120 sentences per second at all lengths (using a beam size of 1). The addition of the attention mechanism also improved the efficiency of the model, since it was able to outperform the Sutskever et al, neural network implementation without data augmentation.

One limitation with the approach was the linearization of the parse trees. They used depth-first traversal to define an invertible procedure. This made it possible to convert the syntactic constituency parsing problem into a sequence-to-sequence problem, which could be solved by a recurrent neural network. However, the model contains no explicit measures to ensure that the linearized parse trees it outputs are well-formed. Indeed, the authors found that for their best model, 14 of the 1700 sentences in their development set the model produced malformed trees. In these cases, brackets were added artificially to balance the parse tree.

Unlike older syntactic constituency parsing algorithms like CYK, the sequence-to-sequence approach is not a dynamic programming algorithm. Therefore it does not require a large number of hand-engineered features like CYK, but it does require the collection of these large training datasets. It also requires an up-front investment of a potentially large amount of compute, while CYK does not require any model training in the traditional sense. However CYK could be considered more interpretable since the rules are explicit and can be reviewed by experts and modified if necessary.

\section{Topics to review}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{\spacedlowsmallcaps{References}} % For modifying the bibliography heading

\bibliographystyle{plainnat}

\bibliography{sample.bib} % The file containing the bibliography

%----------------------------------------------------------------------------------------

\end{document}
