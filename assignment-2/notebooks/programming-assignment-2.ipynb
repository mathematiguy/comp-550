{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878b0ab7-c6b0-4db4-9e7f-2c0437b50984",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a08ba21-ff49-4cad-953f-fc7bc95dbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r ../requirements.txt\n",
    "# ! python3 -m nltk.downloader -d .nltk_data all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a91c0b-d23e-4ba8-a92a-c729517f9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('.nltk_data')\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from loader import *\n",
    "\n",
    "from nltk import wsd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from collections import Counter\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9c418-31dd-4b89-a970-31e116bea65a",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d81e2a-bc47-4170-91d0-f04463b25fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "1450\n"
     ]
    }
   ],
   "source": [
    "data_f = \"../code/multilingual-all-words.en.xml\"\n",
    "key_f = \"../code/wordnet.en.key\"\n",
    "dev_instances, test_instances = load_instances(data_f)\n",
    "dev_key, test_key = load_key(key_f)\n",
    "\n",
    "# IMPORTANT: keys contain fewer entries than the instances; need to remove them\n",
    "dev_instances = {k: v for (k, v) in dev_instances.items() if k in dev_key}\n",
    "test_instances = {k: v for (k, v) in test_instances.items() if k in test_key}\n",
    "\n",
    "# ready to use here\n",
    "print(len(dev_instances))  # number of dev instances\n",
    "print(len(test_instances))  # number of test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff380b56-d627-44e2-93ee-03b9a94bf4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'id', 'index', 'lemma']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(dev_instances[\"d001.s001.t002\"]) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602b304d-1860-4d5a-81cd-7463db2e9127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group%1:03:00::']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_key['d001.s001.t002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b73454b-d868-4664-aa0a-07dc61182f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001.s001.t002\tgroup\tU.N. group draft plan to reduce emission\t1\n"
     ]
    }
   ],
   "source": [
    "print(dev_instances[\"d001.s001.t002\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f1b7fc-9645-4b0f-b696-69e3f8f9111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dev_instances[\"d001.s001.t002\"].context\n",
    "id = dev_instances[\"d001.s001.t002\"].id\n",
    "index = dev_instances[\"d001.s001.t002\"].index\n",
    "lemma = dev_instances[\"d001.s001.t002\"].lemma\n",
    "\n",
    "assert lemma == context[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b28efa-d4f9-4427-aaee-1491943c3e3e",
   "metadata": {},
   "source": [
    "## Look at synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caaaf5d1-e8da-4c8e-a5fd-5f02ed06f05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets('dog')\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fadd2121-6809-4326-b1fc-29975f4fd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wn.synsets(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d109b4-6c24-46f9-87a4-8c5f51882adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: group.n.01\n",
      "Lemmas: ['group', 'grouping']\n",
      "Definition: any number of entities (members) considered as a unit\n",
      "Examples: []\n",
      "\n",
      "Synset: group.n.02\n",
      "Lemmas: ['group', 'radical', 'chemical_group']\n",
      "Definition: (chemistry) two or more atoms bound together as a single unit and forming part of a molecule\n",
      "Examples: []\n",
      "\n",
      "Synset: group.n.03\n",
      "Lemmas: ['group', 'mathematical_group']\n",
      "Definition: a set that is closed, associative, has an identity element and every element has an inverse\n",
      "Examples: []\n",
      "\n",
      "Synset: group.v.01\n",
      "Lemmas: ['group']\n",
      "Definition: arrange into a group or groups\n",
      "Examples: ['Can you group these shapes together?']\n",
      "\n",
      "Synset: group.v.02\n",
      "Lemmas: ['group', 'aggroup']\n",
      "Definition: form a group or group together\n",
      "Examples: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for syn in synsets:\n",
    "    print(\"Synset:\", syn.name())\n",
    "    print(\"Lemmas:\", [lemma.name() for lemma in syn.lemmas()])\n",
    "    print(\"Definition:\", syn.definition())\n",
    "    print(\"Examples:\", syn.examples())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4fb26-b8a3-42bd-8ec9-b9054c2d512c",
   "metadata": {},
   "source": [
    "## Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd79eef-7d35-4800-a058-8bb59bdc2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, stop_words=stop_words, lemmatizer=lemmatizer):\n",
    "    \"\"\"\n",
    "    Preprocesses the sentence by lemmatizing and removing stop words and words without any alphanumeric characters.\n",
    "    Assumes sentence is a list of tokenized words.\n",
    "    \"\"\"\n",
    "    # Helper function to check for at least one alphanumeric character in a word\n",
    "    contains_alnum = lambda word: any(char.isalnum() for char in word)\n",
    "\n",
    "    # Lemmatize words, filter out stop words and words without any alphanumeric characters\n",
    "    processed = {lemmatizer.lemmatize(w) for w in sentence if w not in stop_words and contains_alnum(w)}\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "def lesk(lemma, context):\n",
    "    \"\"\"\n",
    "    Lesk's algorithm implementation.\n",
    "    Assumes preprocessed_context is a set of lemmatized words without stop words.\n",
    "    \"\"\"\n",
    "    assert isinstance(lemma, str), \"Lemma is not a string\"\n",
    "    assert len(context) > 0, \"Empty context\"\n",
    "\n",
    "    max_overlap = 0\n",
    "    best_sense = None\n",
    "\n",
    "    context = preprocess(context)\n",
    "\n",
    "    # Obtain the synsets for the lemma\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    # Default to the most common sense if synsets are available\n",
    "    if synsets:\n",
    "        best_sense = synsets[0]\n",
    "\n",
    "    for sense in synsets:\n",
    "        # Preprocess the signature (definition and examples)\n",
    "        signature = preprocess(word_tokenize(sense.definition()), stop_words, lemmatizer)\n",
    "        for example in sense.examples():\n",
    "            signature |= preprocess(word_tokenize(example), stop_words, lemmatizer)\n",
    "\n",
    "        # The overlap is the size of the intersection\n",
    "        overlap = len(context & signature)\n",
    "\n",
    "        # Keep track of the best overlap so far\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f6d81c-4c98-4b27-a485-29ef938a92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = 'climate'\n",
    "context = 'the U.N.-sponsored climate conference -- characterize so far by unruly posturing and mutual recrimination -- gain renewed focus Friday with the release of a document outline ambitious greenhouse-gas reduction over the next @card@ year , with industrialized_nation shoulder most of the burden in the near term .'.split()\n",
    "best_sense = lesk(lemma, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9cb539d-a7eb-45ab-84ea-20b2469cd1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climate%1:26:00::', 'clime%1:26:00::']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_keys = [lemma.key() for lemma in best_sense.lemmas()]\n",
    "sense_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f824b983-f7c2-45df-ae59-32d06d5bfe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001.s001.t002\tgroup\tU.N. group draft plan to reduce emission\t1\n",
      "arrange into a group or groups\n",
      "\n",
      "d001.s001.t003\tplan\tU.N. group draft plan to reduce emission\t3\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "d001.s001.t004\temission\tU.N. group draft plan to reduce emission\t6\n",
      "the act of emitting; causing to flow forth\n",
      "\n",
      "d001.s002.t001\tclimate\tthe U.N.-sponsored climate conference -- characterize so far by unruly posturing and mutual recrimination -- gain renewed focus Friday with the release of a document outline ambitious greenhouse-gas reduction over the next @card@ year , with industrialized_nation shoulder most of the burden in the near term .\t2\n",
      "the weather in some location averaged over some long period of time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(dev_instances.items()):\n",
    "    key, instance = item\n",
    "    lemma = instance.lemma\n",
    "    context = instance.context\n",
    "    print(instance)\n",
    "    print(lesk(lemma, context).definition())\n",
    "    print()\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69954ef1-5b65-449b-979d-7208f0df43c6",
   "metadata": {},
   "source": [
    "## Lemma sense keys and synset numbers\n",
    "\n",
    "The correspondence between lemma sense keys and synset numbers is stored in `wordnet.en.key`. We can just read from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b251ce6-df0a-4d7e-84cd-6cce1afcbfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001 d001.s001.t002 group%1:03:00:: \n",
      "d001 d001.s001.t003 plan%1:09:00:: \n",
      "d001 d001.s001.t004 emission%1:27:00:: \n",
      "d001 d001.s002.t001 climate%1:26:00:: \n",
      "d001 d001.s002.t002 conference%1:14:00:: \n",
      "d001 d001.s002.t003 posturing%1:07:00:: \n",
      "d001 d001.s002.t004 recrimination%1:10:00:: \n",
      "d001 d001.s002.t005 focus%1:09:00:: \n",
      "d001 d001.s002.t006 friday%1:28:00:: \n",
      "d001 d001.s002.t007 release%1:22:00:: \n"
     ]
    }
   ],
   "source": [
    "! head ../code/wordnet.en.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a09341-be3d-47d8-9e9c-16c2487e6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_sense_key_to_synset_number_correspondence():\n",
    "    wordnet_key_file = '../code/wordnet.en.key'\n",
    "    lsk_to_sn = {}\n",
    "    with open(wordnet_key_file, 'r') as f:\n",
    "        for line in f.read().strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            _, lsk, sn = line.split(' ', 2)\n",
    "            lsk_to_sn[lsk] = set(sn.split(' '))\n",
    "    return lsk_to_sn\n",
    "\n",
    "lsk_to_sn = get_lemma_sense_key_to_synset_number_correspondence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3f93881-f3e9-4017-98f2-4cddb5f67a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d001.s001.t002': {'group%1:03:00::'},\n",
       " 'd001.s001.t003': {'plan%1:09:00::'},\n",
       " 'd001.s001.t004': {'emission%1:27:00::'},\n",
       " 'd001.s002.t001': {'climate%1:26:00::'},\n",
       " 'd001.s002.t002': {'conference%1:14:00::'},\n",
       " 'd001.s002.t003': {'posturing%1:07:00::'},\n",
       " 'd001.s002.t004': {'recrimination%1:10:00::'},\n",
       " 'd001.s002.t005': {'focus%1:09:00::'},\n",
       " 'd001.s002.t006': {'friday%1:28:00::'},\n",
       " 'd001.s002.t007': {'release%1:22:00::'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top ten\n",
    "{x[0]: x[1] for i, x in enumerate(lsk_to_sn.items()) if i < 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cb7f5-901e-46fb-98f6-4e263506cdcf",
   "metadata": {},
   "source": [
    "## Calculating accuracy\n",
    "\n",
    "Now that I have this correspondence, I can calculate the accuracy of my lesk's algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1295f176-5ab8-4981-96b1-d8f926555efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wsd_data():\n",
    "    wsd_data = []\n",
    "    for key, instance in dev_instances.items():\n",
    "        id = instance.id\n",
    "        lemma = instance.lemma\n",
    "        context = instance.context\n",
    "    \n",
    "        processed_context = preprocess(context)\n",
    "    \n",
    "        # Use lesk's algorithm to guess the synset\n",
    "        synset = lesk(lemma, context)\n",
    "    \n",
    "        # Get the sense-keys for the predicted synset\n",
    "        preds = set(lemma.key() for lemma in synset.lemmas())\n",
    "        \n",
    "        # Extract the synset number from the sense-key\n",
    "        targets = lsk_to_sn[id]\n",
    "        \n",
    "        # Calculate if there is any overlap between the predicted sense and the target\n",
    "        match = len(preds & targets) > 0\n",
    "        \n",
    "        wsd_data.append(\n",
    "            dict(id=id, lemma=lemma, context=context, processed_context=processed_context, synset=synset, preds=preds, targets=targets, match=match)\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(wsd_data)\n",
    "\n",
    "wsd_data = build_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6456356c-bf2f-4903-b145-f5a43ce62470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d001.s001.t002</td>\n",
       "      <td>group</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('group.v.01')</td>\n",
       "      <td>{group%2:31:00::}</td>\n",
       "      <td>{group%1:03:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d001.s001.t003</td>\n",
       "      <td>plan</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('plan.n.01')</td>\n",
       "      <td>{program%1:09:00::, programme%1:09:00::, plan%...</td>\n",
       "      <td>{plan%1:09:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d001.s001.t004</td>\n",
       "      <td>emission</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('emission.n.01')</td>\n",
       "      <td>{emanation%1:04:00::, emission%1:04:00::}</td>\n",
       "      <td>{emission%1:27:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d001.s002.t001</td>\n",
       "      <td>climate</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{outline, characterize, conference, climate, n...</td>\n",
       "      <td>Synset('climate.n.01')</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d001.s002.t002</td>\n",
       "      <td>conference</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{outline, characterize, conference, climate, n...</td>\n",
       "      <td>Synset('conference.n.01')</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       lemma  \\\n",
       "0  d001.s001.t002       group   \n",
       "1  d001.s001.t003        plan   \n",
       "2  d001.s001.t004    emission   \n",
       "3  d001.s002.t001     climate   \n",
       "4  d001.s002.t002  conference   \n",
       "\n",
       "                                             context  \\\n",
       "0   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "1   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "2   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "3  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "4  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "\n",
       "                                   processed_context  \\\n",
       "0       {emission, group, U.N., plan, draft, reduce}   \n",
       "1       {emission, group, U.N., plan, draft, reduce}   \n",
       "2       {emission, group, U.N., plan, draft, reduce}   \n",
       "3  {outline, characterize, conference, climate, n...   \n",
       "4  {outline, characterize, conference, climate, n...   \n",
       "\n",
       "                      synset  \\\n",
       "0       Synset('group.v.01')   \n",
       "1        Synset('plan.n.01')   \n",
       "2    Synset('emission.n.01')   \n",
       "3     Synset('climate.n.01')   \n",
       "4  Synset('conference.n.01')   \n",
       "\n",
       "                                               preds                 targets  \\\n",
       "0                                  {group%2:31:00::}       {group%1:03:00::}   \n",
       "1  {program%1:09:00::, programme%1:09:00::, plan%...        {plan%1:09:00::}   \n",
       "2          {emanation%1:04:00::, emission%1:04:00::}    {emission%1:27:00::}   \n",
       "3               {climate%1:26:00::, clime%1:26:00::}     {climate%1:26:00::}   \n",
       "4                             {conference%1:14:00::}  {conference%1:14:00::}   \n",
       "\n",
       "   match  \n",
       "0  False  \n",
       "1   True  \n",
       "2  False  \n",
       "3   True  \n",
       "4   True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c92099ed-2ff9-464b-958c-75e2484b1ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      d001.s001.t003\n",
       "lemma                                                             plan\n",
       "context               [U.N., group, draft, plan, to, reduce, emission]\n",
       "processed_context         {emission, group, U.N., plan, draft, reduce}\n",
       "synset                                             Synset('plan.n.01')\n",
       "preds                {program%1:09:00::, programme%1:09:00::, plan%...\n",
       "targets                                               {plan%1:09:00::}\n",
       "match                                                             True\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54905c5b-1bd7-4546-ae73-6f044c001bad",
   "metadata": {},
   "source": [
    "I'm checking that the preprocessing step correctly resolves multi-word phrases to single entities and it appears it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca1b206-abb4-49e3-9567-da1997f533c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>d001.s016.t001</td>\n",
       "      <td>comment</td>\n",
       "      <td>[Stern, make, his, comment, an, hour, after, C...</td>\n",
       "      <td>{common_sense, climate, help, make, lack, fina...</td>\n",
       "      <td>Synset('comment.v.01')</td>\n",
       "      <td>{point_out%2:32:01::, comment%2:32:00::, notic...</td>\n",
       "      <td>{comment%1:10:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>d001.s016.t002</td>\n",
       "      <td>hour</td>\n",
       "      <td>[Stern, make, his, comment, an, hour, after, C...</td>\n",
       "      <td>{common_sense, climate, help, make, lack, fina...</td>\n",
       "      <td>Synset('hour.n.01')</td>\n",
       "      <td>{hr%1:28:00::, 60_minutes%1:28:00::, hour%1:28...</td>\n",
       "      <td>{hour%1:28:01::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id    lemma  \\\n",
       "99   d001.s016.t001  comment   \n",
       "100  d001.s016.t002     hour   \n",
       "\n",
       "                                               context  \\\n",
       "99   [Stern, make, his, comment, an, hour, after, C...   \n",
       "100  [Stern, make, his, comment, an, hour, after, C...   \n",
       "\n",
       "                                     processed_context  \\\n",
       "99   {common_sense, climate, help, make, lack, fina...   \n",
       "100  {common_sense, climate, help, make, lack, fina...   \n",
       "\n",
       "                     synset  \\\n",
       "99   Synset('comment.v.01')   \n",
       "100     Synset('hour.n.01')   \n",
       "\n",
       "                                                 preds              targets  \\\n",
       "99   {point_out%2:32:01::, comment%2:32:00::, notic...  {comment%1:10:00::}   \n",
       "100  {hr%1:28:00::, 60_minutes%1:28:00::, hour%1:28...     {hour%1:28:01::}   \n",
       "\n",
       "     match  \n",
       "99   False  \n",
       "100  False  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data[wsd_data['processed_context'].apply(lambda x: 'America' in x)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e5d8753-31b7-4d1c-ad2a-3cd840b5d76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Stern', 'make', 'his', 'comment', 'an', 'hour', 'after', 'Chinese', 'vice', 'foreign_minister', 'he_yafei', 'say', 'America', \"'s\", 'top', 'climate', 'negotiator', 'be', 'either', 'lack', '``', 'common_sense', \"''\", 'or', 'be', '``', 'extremely', 'irresponsible', \"''\", 'for', 'say', 'earlier', 'in', 'the', 'week', 'that', 'the', 'united_states', 'would', 'not', 'help', 'China', 'financially', 'to', 'cope', 'with', 'global_warming', '.']),\n",
       "       {'common_sense', 'climate', 'help', 'make', 'lack', 'financially', 'Stern', 'negotiator', 'foreign_minister', 'extremely', 'irresponsible', 'China', 'cope', 'America', 'would', 'hour', 'earlier', 'vice', \"'s\", 'comment', 'global_warming', 'united_states', 'top', 'either', 'say', 'Chinese', 'week', 'he_yafei'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.loc[101, ['context', 'processed_context']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f237a7c-b8f7-4f77-845a-737474c7cafa",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dbbb464-9d8c-4097-aa92-fe9b267884f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk's algorithm Accuracy: 57.2%\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk\\'s algorithm Accuracy: {100*wsd_data.match.mean():.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11557122-da15-4a2f-8b70-3a31d3416a6a",
   "metadata": {},
   "source": [
    "This seems like a decent score for this task considering the effort we put in.\n",
    "\n",
    "Next we should calculate scores for:\n",
    "- The most frequent sense baseline\n",
    "- NLTK's implementation of Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8643bac-1260-4fb4-b177-79751ece525a",
   "metadata": {},
   "source": [
    "## Most frequent sense baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f55a109a-55be-46fc-9846-b429a8e789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_synset(lemma):\n",
    "    '''\n",
    "    Given a lemma, this returns the most frequent sense for that lemma.\n",
    "    '''\n",
    "    return set(lemma.key() for lemma in wn.synsets(lemma)[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1af565e-19d5-4c42-acae-40d664633ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_data['most_frequent_synset'] = wsd_data.lemma.apply(most_frequent_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279da987-25a9-4c0d-8c68-d2f1ca388065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Synset Accuracy: 67.5%\n"
     ]
    }
   ],
   "source": [
    "most_frequent_synset_accuracy = wsd_data.apply(lambda x: len(x.most_frequent_synset & x.targets) > 0, axis=1).mean()\n",
    "print(f'Most Frequent Synset Accuracy: {100*most_frequent_synset_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de6259-5c0c-40b9-ad22-90a4485d0e9e",
   "metadata": {},
   "source": [
    "## NLTK Lesk Algorithm baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f31c7b2-3860-4308-8f76-3f372fc54f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lesk's Algorithm Accuracy: 34.0%\n"
     ]
    }
   ],
   "source": [
    "wsd_data['nltk_pred_synset'] = wsd_data.apply(lambda x: set(lemma.key() for lemma in wsd.lesk(x.context, x.lemma).lemmas()), axis = 1)\n",
    "nltk_pred_synset_accuracy = wsd_data.apply(lambda x: len(x.nltk_pred_synset & x.targets) > 0, axis=1).mean()\n",
    "print(f'NLTK Lesk\\'s Algorithm Accuracy: {100*nltk_pred_synset_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96d6c8c3-eccd-4652-b0c5-bc6cf9a9fb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "      <th>most_frequent_synset</th>\n",
       "      <th>nltk_pred_synset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d001.s001.t002</td>\n",
       "      <td>group</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('group.v.01')</td>\n",
       "      <td>{group%2:31:00::}</td>\n",
       "      <td>{group%1:03:00::}</td>\n",
       "      <td>False</td>\n",
       "      <td>{grouping%1:03:00::, group%1:03:00::}</td>\n",
       "      <td>{group%2:33:00::, aggroup%2:33:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d001.s001.t003</td>\n",
       "      <td>plan</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('plan.n.01')</td>\n",
       "      <td>{program%1:09:00::, programme%1:09:00::, plan%...</td>\n",
       "      <td>{plan%1:09:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{program%1:09:00::, programme%1:09:00::, plan%...</td>\n",
       "      <td>{design%2:36:02::, project%2:36:01::, plan%2:3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d001.s001.t004</td>\n",
       "      <td>emission</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{emission, group, U.N., plan, draft, reduce}</td>\n",
       "      <td>Synset('emission.n.01')</td>\n",
       "      <td>{emanation%1:04:00::, emission%1:04:00::}</td>\n",
       "      <td>{emission%1:27:00::}</td>\n",
       "      <td>False</td>\n",
       "      <td>{emanation%1:04:00::, emission%1:04:00::}</td>\n",
       "      <td>{emanation%1:04:00::, emission%1:04:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d001.s002.t001</td>\n",
       "      <td>climate</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{outline, characterize, conference, climate, n...</td>\n",
       "      <td>Synset('climate.n.01')</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d001.s002.t002</td>\n",
       "      <td>conference</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{outline, characterize, conference, climate, n...</td>\n",
       "      <td>Synset('conference.n.01')</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       lemma  \\\n",
       "0  d001.s001.t002       group   \n",
       "1  d001.s001.t003        plan   \n",
       "2  d001.s001.t004    emission   \n",
       "3  d001.s002.t001     climate   \n",
       "4  d001.s002.t002  conference   \n",
       "\n",
       "                                             context  \\\n",
       "0   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "1   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "2   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "3  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "4  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "\n",
       "                                   processed_context  \\\n",
       "0       {emission, group, U.N., plan, draft, reduce}   \n",
       "1       {emission, group, U.N., plan, draft, reduce}   \n",
       "2       {emission, group, U.N., plan, draft, reduce}   \n",
       "3  {outline, characterize, conference, climate, n...   \n",
       "4  {outline, characterize, conference, climate, n...   \n",
       "\n",
       "                      synset  \\\n",
       "0       Synset('group.v.01')   \n",
       "1        Synset('plan.n.01')   \n",
       "2    Synset('emission.n.01')   \n",
       "3     Synset('climate.n.01')   \n",
       "4  Synset('conference.n.01')   \n",
       "\n",
       "                                               preds                 targets  \\\n",
       "0                                  {group%2:31:00::}       {group%1:03:00::}   \n",
       "1  {program%1:09:00::, programme%1:09:00::, plan%...        {plan%1:09:00::}   \n",
       "2          {emanation%1:04:00::, emission%1:04:00::}    {emission%1:27:00::}   \n",
       "3               {climate%1:26:00::, clime%1:26:00::}     {climate%1:26:00::}   \n",
       "4                             {conference%1:14:00::}  {conference%1:14:00::}   \n",
       "\n",
       "   match                               most_frequent_synset  \\\n",
       "0  False              {grouping%1:03:00::, group%1:03:00::}   \n",
       "1   True  {program%1:09:00::, programme%1:09:00::, plan%...   \n",
       "2  False          {emanation%1:04:00::, emission%1:04:00::}   \n",
       "3   True               {climate%1:26:00::, clime%1:26:00::}   \n",
       "4   True                             {conference%1:14:00::}   \n",
       "\n",
       "                                    nltk_pred_synset  \n",
       "0               {group%2:33:00::, aggroup%2:33:00::}  \n",
       "1  {design%2:36:02::, project%2:36:01::, plan%2:3...  \n",
       "2          {emanation%1:04:00::, emission%1:04:00::}  \n",
       "3               {climate%1:26:00::, clime%1:26:00::}  \n",
       "4                             {conference%1:14:00::}  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5652b42-0a62-42b5-9b97-fb6339a9fb07",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Now we'll use the SemCor corpus to apply Yarowsky's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55ba5512-6f61-43ac-b1a2-82743a55aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 829 ms, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_semcor_data():\n",
    "    '''\n",
    "    Uses the semcor corpus to assemble a dataset for bootstrap classification\n",
    "    '''\n",
    "    semcor_data = []\n",
    "    for sent_id, tagged_sent in enumerate(semcor.tagged_sents(tag='sense')):\n",
    "        sentence = [\"_\".join(tree.leaves()) for tree in tagged_sent]\n",
    "        for i, tree in enumerate(tagged_sent):\n",
    "            d = {}\n",
    "            d['sentence'] = sentence\n",
    "            d['sentence_text'] = ' '.join(sentence)\n",
    "            d['processed_sentence'] = preprocess(sentence)\n",
    "            d['span'] = sentence[i]\n",
    "    \n",
    "            if hasattr(tree, 'label') and isinstance(tree.label(), Lemma):\n",
    "                d['sense_key'] = tree.label().key()\n",
    "            else:\n",
    "                continue\n",
    "            semcor_data.append(d)\n",
    "    \n",
    "    return pd.DataFrame(semcor_data)\n",
    "\n",
    "semcor_data = build_semcor_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a22f4a19-b51f-4207-9ca3-f6f8bda9a47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>said</td>\n",
       "      <td>say%2:32:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>investigation%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>atlanta%1:15:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224711</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>stung</td>\n",
       "      <td>sting%2:39:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224712</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>was</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224713</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>let</td>\n",
       "      <td>let%2:41:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224714</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>make</td>\n",
       "      <td>make%2:41:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224715</th>\n",
       "      <td>[``, I, can, n't, turn, the, studio, into, a, ...</td>\n",
       "      <td>`` I can n't turn the studio into a gambling_h...</td>\n",
       "      <td>{I, studio, gambling_hell, n't, said, turn, sa...</td>\n",
       "      <td>said</td>\n",
       "      <td>say%2:32:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "224711  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224712  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224713  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224714  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224715  [``, I, can, n't, turn, the, studio, into, a, ...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "224711  Her reply stung me , but this was too importan...   \n",
       "224712  Her reply stung me , but this was too importan...   \n",
       "224713  Her reply stung me , but this was too importan...   \n",
       "224714  Her reply stung me , but this was too importan...   \n",
       "224715  `` I can n't turn the studio into a gambling_h...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {recent, primary_election, evidence, Friday, F...   \n",
       "1       {recent, primary_election, evidence, Friday, F...   \n",
       "2       {recent, primary_election, evidence, Friday, F...   \n",
       "3       {recent, primary_election, evidence, Friday, F...   \n",
       "4       {recent, primary_election, evidence, Friday, F...   \n",
       "...                                                   ...   \n",
       "224711  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224712  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224713  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224714  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224715  {I, studio, gambling_hell, n't, said, turn, sa...   \n",
       "\n",
       "                            span                sense_key  \n",
       "0       Fulton_County_Grand_Jury          group%1:03:00::  \n",
       "1                           said            say%2:32:00::  \n",
       "2                         Friday         friday%1:28:00::  \n",
       "3                  investigation  investigation%1:09:00::  \n",
       "4                        Atlanta        atlanta%1:15:00::  \n",
       "...                          ...                      ...  \n",
       "224711                     stung          sting%2:39:02::  \n",
       "224712                       was             be%2:42:03::  \n",
       "224713                       let            let%2:41:00::  \n",
       "224714                      make           make%2:41:00::  \n",
       "224715                      said            say%2:32:00::  \n",
       "\n",
       "[224716 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d60ac8-171f-46ca-a88e-e810230c1d75",
   "metadata": {},
   "source": [
    "## Blank out lemma sense keys\n",
    "\n",
    "We need to replace the lemma sense keys that are not in our dev/test set with None. We keep the processed sentences, so that when we form term-document matrices we still have all of the tokens but we will only predict cases where the sense_key is valid and in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bab4f246-25e9-4d8a-a8d8-6cf39e803182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224711</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>stung</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224712</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>was</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224713</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>let</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224714</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{difference, reply, hurt, make, stung, Her, le...</td>\n",
       "      <td>make</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224715</th>\n",
       "      <td>[``, I, can, n't, turn, the, studio, into, a, ...</td>\n",
       "      <td>`` I can n't turn the studio into a gambling_h...</td>\n",
       "      <td>{I, studio, gambling_hell, n't, said, turn, sa...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "224711  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224712  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224713  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224714  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224715  [``, I, can, n't, turn, the, studio, into, a, ...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "224711  Her reply stung me , but this was too importan...   \n",
       "224712  Her reply stung me , but this was too importan...   \n",
       "224713  Her reply stung me , but this was too importan...   \n",
       "224714  Her reply stung me , but this was too importan...   \n",
       "224715  `` I can n't turn the studio into a gambling_h...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {recent, primary_election, evidence, Friday, F...   \n",
       "1       {recent, primary_election, evidence, Friday, F...   \n",
       "2       {recent, primary_election, evidence, Friday, F...   \n",
       "3       {recent, primary_election, evidence, Friday, F...   \n",
       "4       {recent, primary_election, evidence, Friday, F...   \n",
       "...                                                   ...   \n",
       "224711  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224712  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224713  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224714  {difference, reply, hurt, make, stung, Her, le...   \n",
       "224715  {I, studio, gambling_hell, n't, said, turn, sa...   \n",
       "\n",
       "                            span         sense_key  \n",
       "0       Fulton_County_Grand_Jury   group%1:03:00::  \n",
       "1                           said              None  \n",
       "2                         Friday  friday%1:28:00::  \n",
       "3                  investigation              None  \n",
       "4                        Atlanta              None  \n",
       "...                          ...               ...  \n",
       "224711                     stung              None  \n",
       "224712                       was              None  \n",
       "224713                       let              None  \n",
       "224714                      make              None  \n",
       "224715                      said              None  \n",
       "\n",
       "[224716 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sense_keys = set([k for v in list(dev_key.values()) + list(test_key.values()) for k in v])\n",
    "semcor_data['sense_key'] = semcor_data.sense_key.apply(lambda x: x if x in lemma_sense_keys else None)\n",
    "semcor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3847c736-6149-42d5-ab23-e40886198b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 14,114 valid sentence-sense_key pairs in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {(~semcor_data.sense_key.isnull()).sum():,d} valid sentence-sense_key pairs in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0a7bb-5871-4860-bada-0741922cc351",
   "metadata": {},
   "source": [
    "## Calculate lemma sense ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd914951-ca9d-4742-bcc3-35f161dc56f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "\n",
       "                                  processed_sentence  \\\n",
       "0  {recent, primary_election, evidence, Friday, F...   \n",
       "1  {recent, primary_election, evidence, Friday, F...   \n",
       "2  {recent, primary_election, evidence, Friday, F...   \n",
       "3  {recent, primary_election, evidence, Friday, F...   \n",
       "4  {recent, primary_election, evidence, Friday, F...   \n",
       "\n",
       "                       span         sense_key  sense_id  \n",
       "0  Fulton_County_Grand_Jury   group%1:03:00::     305.0  \n",
       "1                      said              None       NaN  \n",
       "2                    Friday  friday%1:28:00::     288.0  \n",
       "3             investigation              None       NaN  \n",
       "4                   Atlanta              None       NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sense_keys = sorted(lemma_sense_keys)\n",
    "lemma_sense_key_to_id = {v: k for k, v in enumerate(lemma_sense_keys)}\n",
    "\n",
    "semcor_data['sense_id'] = semcor_data.sense_key.apply(lambda x: lemma_sense_key_to_id[x] if x else None)\n",
    "semcor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb90eb-25fa-490b-b56c-2b4106a4f60f",
   "metadata": {},
   "source": [
    "## Drop sentences where no words have a valid sense key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a80c2c06-9555-4983-9f54-8e7b086b514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_sentences = (semcor_data\n",
    "    .groupby('sentence_text')['sense_key']\n",
    "    .apply(lambda x: ~x.isna().all())\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "keep_sentences = (keep_sentences\n",
    "    .loc[keep_sentences['sense_key'], ['sentence_text']]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "keep_sentences = set(keep_sentences.sentence_text.values)\n",
    "\n",
    "semcor_data = semcor_data[semcor_data.sentence_text.isin(keep_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e19b5c-858a-4925-ac9e-178928c29ee5",
   "metadata": {},
   "source": [
    "## Vocab List\n",
    "\n",
    "We now have enough to start making a model. We can start by constructing a vocab list and document matrices etc so we can formulate the problem as a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e56bfed-a83d-433e-be89-a8fe85a8b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = semcor_data.span.value_counts(ascending=True).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cffd6516-f09f-43a9-82b7-380bef220e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cackly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wails</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mourning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shelves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21737</th>\n",
       "      <td>are</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21738</th>\n",
       "      <td>be</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21739</th>\n",
       "      <td>not</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21740</th>\n",
       "      <td>was</td>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21741</th>\n",
       "      <td>is</td>\n",
       "      <td>1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21742 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           span  count\n",
       "0      Fulton_County_Grand_Jury      1\n",
       "1                        cackly      1\n",
       "2                         wails      1\n",
       "3                      mourning      1\n",
       "4                       shelves      1\n",
       "...                         ...    ...\n",
       "21737                       are    544\n",
       "21738                        be    616\n",
       "21739                       not    751\n",
       "21740                       was   1021\n",
       "21741                        is   1385\n",
       "\n",
       "[21742 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf30b197-1ec4-43ba-8aa1-104ca1ad3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_to_id = {span: id for id, span in enumerate(vocab_data.span)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6307d4-578d-4dfd-833c-217416dc4734",
   "metadata": {},
   "source": [
    "## Calculate document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1259fc4c-f1b4-4b70-b7b1-138000891736",
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_data = semcor_data.copy()\n",
    "semcor_data.loc[:, 'document_vector'] = semcor_data.loc[:, 'sentence'].apply(lambda x: [span_to_id.get(span) for span in x if span in span_to_id.keys()])\n",
    "semcor_data.loc[:, 'span_id'] = semcor_data.loc[:, 'span'].apply(lambda x: span_to_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28a04d07-27ed-44ec-b5f6-1d5249950a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>document_vector</th>\n",
       "      <th>span_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305.0</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>21725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288.0</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>20585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>19237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>19277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183966</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>get</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183967</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>things</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183968</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>done</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183969</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>months</td>\n",
       "      <td>month%1:28:01::</td>\n",
       "      <td>447.0</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183970</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102369 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "183966  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "183967  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "183968  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "183969  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "183970  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "183966  `` I 've been waiting to get these things done...   \n",
       "183967  `` I 've been waiting to get these things done...   \n",
       "183968  `` I 've been waiting to get these things done...   \n",
       "183969  `` I 've been waiting to get these things done...   \n",
       "183970  `` I 've been waiting to get these things done...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {recent, primary_election, evidence, Friday, F...   \n",
       "1       {recent, primary_election, evidence, Friday, F...   \n",
       "2       {recent, primary_election, evidence, Friday, F...   \n",
       "3       {recent, primary_election, evidence, Friday, F...   \n",
       "4       {recent, primary_election, evidence, Friday, F...   \n",
       "...                                                   ...   \n",
       "183966   {month, get, 've, I, waiting, done, thing, said}   \n",
       "183967   {month, get, 've, I, waiting, done, thing, said}   \n",
       "183968   {month, get, 've, I, waiting, done, thing, said}   \n",
       "183969   {month, get, 've, I, waiting, done, thing, said}   \n",
       "183970   {month, get, 've, I, waiting, done, thing, said}   \n",
       "\n",
       "                            span         sense_key  sense_id  \\\n",
       "0       Fulton_County_Grand_Jury   group%1:03:00::     305.0   \n",
       "1                           said              None       NaN   \n",
       "2                         Friday  friday%1:28:00::     288.0   \n",
       "3                  investigation              None       NaN   \n",
       "4                        Atlanta              None       NaN   \n",
       "...                          ...               ...       ...   \n",
       "183966                       get              None       NaN   \n",
       "183967                    things              None       NaN   \n",
       "183968                      done              None       NaN   \n",
       "183969                    months   month%1:28:01::     447.0   \n",
       "183970                      said              None       NaN   \n",
       "\n",
       "                                          document_vector  span_id  \n",
       "0       [0, 21725, 20585, 19237, 19277, 21232, 21270, ...        0  \n",
       "1       [0, 21725, 20585, 19237, 19277, 21232, 21270, ...    21725  \n",
       "2       [0, 21725, 20585, 19237, 19277, 21232, 21270, ...    20585  \n",
       "3       [0, 21725, 20585, 19237, 19277, 21232, 21270, ...    19237  \n",
       "4       [0, 21725, 20585, 19237, 19277, 21232, 21270, ...    19277  \n",
       "...                                                   ...      ...  \n",
       "183966  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21622  \n",
       "183967  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21650  \n",
       "183968  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21504  \n",
       "183969  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21527  \n",
       "183970  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21725  \n",
       "\n",
       "[102369 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d27376-6c7a-4a3f-b775-ee92dcc3c304",
   "metadata": {},
   "source": [
    "Now that we have the term-document vectors we can drop the rows where the sense keys are not in our target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e15902-9d08-4318-aa2e-78b9ea4feb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>document_vector</th>\n",
       "      <th>span_id</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305.0</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{recent, primary_election, evidence, Friday, F...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288.0</td>\n",
       "      <td>[0, 21725, 20585, 19237, 19277, 21232, 21270, ...</td>\n",
       "      <td>20585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, jury, election, over-all, charge, Cit...</td>\n",
       "      <td>jury</td>\n",
       "      <td>jury%1:14:00::</td>\n",
       "      <td>378.0</td>\n",
       "      <td>[20890, 21158, 21725, 20022, 20886, 21635, 103...</td>\n",
       "      <td>20890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, jury, election, over-all, charge, Cit...</td>\n",
       "      <td>term</td>\n",
       "      <td>term%1:28:00::</td>\n",
       "      <td>728.0</td>\n",
       "      <td>[20890, 21158, 21725, 20022, 20886, 21635, 103...</td>\n",
       "      <td>20886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, jury, election, over-all, charge, Cit...</td>\n",
       "      <td>end</td>\n",
       "      <td>end%1:28:00::</td>\n",
       "      <td>234.0</td>\n",
       "      <td>[20890, 21158, 21725, 20022, 20886, 21635, 103...</td>\n",
       "      <td>21635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14109</th>\n",
       "      <td>[Without, further, discussion, he, appeared, t...</td>\n",
       "      <td>Without further discussion he appeared the nex...</td>\n",
       "      <td>{light, board, pile, sticking, discussion, nex...</td>\n",
       "      <td>morning</td>\n",
       "      <td>morning%1:28:00::</td>\n",
       "      <td>450.0</td>\n",
       "      <td>[21158, 21193, 21245, 20650, 21601, 1741, 1661...</td>\n",
       "      <td>21601</td>\n",
       "      <td>8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14110</th>\n",
       "      <td>[Before, you, use, ', em, the, light, company,...</td>\n",
       "      <td>Before you use ' em the light company 's got t...</td>\n",
       "      <td>{light, got, fuse, use, box, extra, circuit, n...</td>\n",
       "      <td>company</td>\n",
       "      <td>company%1:14:01::</td>\n",
       "      <td>125.0</td>\n",
       "      <td>[21699, 21491, 21546, 21232, 21461, 21194, 200...</td>\n",
       "      <td>21546</td>\n",
       "      <td>8351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14111</th>\n",
       "      <td>[He, oughta, be, able, to, build, a, new, hous...</td>\n",
       "      <td>He oughta be able to build a new house with al...</td>\n",
       "      <td>{He, house, build, new, oughta, able, contrapt...</td>\n",
       "      <td>house</td>\n",
       "      <td>house%1:06:00::</td>\n",
       "      <td>332.0</td>\n",
       "      <td>[21738, 21287, 20014, 1741, 21720, 21695, 2168...</td>\n",
       "      <td>21695</td>\n",
       "      <td>8352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14112</th>\n",
       "      <td>[Mr._Crombie, watched, his, wife, with, an, an...</td>\n",
       "      <td>Mr._Crombie watched his wife with an anxious e...</td>\n",
       "      <td>{wife, watched, expression, anxious, Mr._Crombie}</td>\n",
       "      <td>wife</td>\n",
       "      <td>wife%1:18:00::</td>\n",
       "      <td>807.0</td>\n",
       "      <td>[17405, 20817, 21686, 14092, 20658]</td>\n",
       "      <td>21686</td>\n",
       "      <td>8353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14113</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, get, 've, I, waiting, done, thing, said}</td>\n",
       "      <td>months</td>\n",
       "      <td>month%1:28:01::</td>\n",
       "      <td>447.0</td>\n",
       "      <td>[21712, 20521, 21622, 21650, 21504, 21527, 21725]</td>\n",
       "      <td>21527</td>\n",
       "      <td>8354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14114 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1      [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2      [The, jury, further, said, in, term, end, pres...   \n",
       "3      [The, jury, further, said, in, term, end, pres...   \n",
       "4      [The, jury, further, said, in, term, end, pres...   \n",
       "...                                                  ...   \n",
       "14109  [Without, further, discussion, he, appeared, t...   \n",
       "14110  [Before, you, use, ', em, the, light, company,...   \n",
       "14111  [He, oughta, be, able, to, build, a, new, hous...   \n",
       "14112  [Mr._Crombie, watched, his, wife, with, an, an...   \n",
       "14113  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "\n",
       "                                           sentence_text  \\\n",
       "0      The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1      The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2      The jury further said in term end presentments...   \n",
       "3      The jury further said in term end presentments...   \n",
       "4      The jury further said in term end presentments...   \n",
       "...                                                  ...   \n",
       "14109  Without further discussion he appeared the nex...   \n",
       "14110  Before you use ' em the light company 's got t...   \n",
       "14111  He oughta be able to build a new house with al...   \n",
       "14112  Mr._Crombie watched his wife with an anxious e...   \n",
       "14113  `` I 've been waiting to get these things done...   \n",
       "\n",
       "                                      processed_sentence  \\\n",
       "0      {recent, primary_election, evidence, Friday, F...   \n",
       "1      {recent, primary_election, evidence, Friday, F...   \n",
       "2      {praise, jury, election, over-all, charge, Cit...   \n",
       "3      {praise, jury, election, over-all, charge, Cit...   \n",
       "4      {praise, jury, election, over-all, charge, Cit...   \n",
       "...                                                  ...   \n",
       "14109  {light, board, pile, sticking, discussion, nex...   \n",
       "14110  {light, got, fuse, use, box, extra, circuit, n...   \n",
       "14111  {He, house, build, new, oughta, able, contrapt...   \n",
       "14112  {wife, watched, expression, anxious, Mr._Crombie}   \n",
       "14113   {month, get, 've, I, waiting, done, thing, said}   \n",
       "\n",
       "                           span          sense_key  sense_id  \\\n",
       "0      Fulton_County_Grand_Jury    group%1:03:00::     305.0   \n",
       "1                        Friday   friday%1:28:00::     288.0   \n",
       "2                          jury     jury%1:14:00::     378.0   \n",
       "3                          term     term%1:28:00::     728.0   \n",
       "4                           end      end%1:28:00::     234.0   \n",
       "...                         ...                ...       ...   \n",
       "14109                   morning  morning%1:28:00::     450.0   \n",
       "14110                   company  company%1:14:01::     125.0   \n",
       "14111                     house    house%1:06:00::     332.0   \n",
       "14112                      wife     wife%1:18:00::     807.0   \n",
       "14113                    months    month%1:28:01::     447.0   \n",
       "\n",
       "                                         document_vector  span_id  sentence_id  \n",
       "0      [0, 21725, 20585, 19237, 19277, 21232, 21270, ...        0            0  \n",
       "1      [0, 21725, 20585, 19237, 19277, 21232, 21270, ...    20585            0  \n",
       "2      [20890, 21158, 21725, 20022, 20886, 21635, 103...    20890            1  \n",
       "3      [20890, 21158, 21725, 20022, 20886, 21635, 103...    20886            1  \n",
       "4      [20890, 21158, 21725, 20022, 20886, 21635, 103...    21635            1  \n",
       "...                                                  ...      ...          ...  \n",
       "14109  [21158, 21193, 21245, 20650, 21601, 1741, 1661...    21601         8350  \n",
       "14110  [21699, 21491, 21546, 21232, 21461, 21194, 200...    21546         8351  \n",
       "14111  [21738, 21287, 20014, 1741, 21720, 21695, 2168...    21695         8352  \n",
       "14112                [17405, 20817, 21686, 14092, 20658]    21686         8353  \n",
       "14113  [21712, 20521, 21622, 21650, 21504, 21527, 21725]    21527         8354  \n",
       "\n",
       "[14114 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data = semcor_data.copy()\n",
    "semcor_data = semcor_data[~semcor_data.sense_key.isnull()]\n",
    "sentence_to_id = {v: k for k, v in enumerate(semcor_data.sentence_text.unique())}\n",
    "semcor_data.loc[:, 'sentence_id'] = semcor_data.loc[:, 'sentence_text'].apply(lambda x: sentence_to_id[x])\n",
    "\n",
    "semcor_data.loc[:, 'sense_id'] = semcor_data.sense_id.astype(np.int32)\n",
    "semcor_data = semcor_data.reset_index(drop=True)\n",
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53600682-264e-4c6f-b2b0-6bbc735bde62",
   "metadata": {},
   "source": [
    "## Convert to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d19e6e10-9a58-48d5-b96e-705a07bd9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(document_vectors):\n",
    "    \n",
    "    # Flatten the Series to create a list of (doc_id, word_id) tuples\n",
    "    rows, cols = zip(*((id, token) for id, doc in enumerate(document_vectors) for token in doc))\n",
    "    \n",
    "    # Create a sparse matrix\n",
    "    # The shape parameters (n_rows, n_cols) should match your data dimensions\n",
    "    n_rows = len(semcor_data)\n",
    "    n_cols = semcor_data.document_vector.apply(max).max() + 1  # assuming token IDs start from 0\n",
    "    data = [1] * len(rows)  # assuming a count of 1 for each occurrence\n",
    "    term_document_matrix = sparse.coo_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "    \n",
    "    # Convert to CSR format for efficient arithmetic and matrix-vector operations\n",
    "    return term_document_matrix.tocsr()\n",
    "\n",
    "term_document_matrix = create_term_document_matrix(semcor_data.document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "296723c2-e3d9-45a9-ad69-44a416da12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data - replace with your actual data\n",
    "X = normalize(term_document_matrix, axis=1, norm='l2')\n",
    "y = semcor_data.sense_id\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1217f95-56aa-4742-bc86-79b52f4e9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from logistic.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "\n",
    "# Define the path for the model\n",
    "model_path = 'logistic.joblib'\n",
    "\n",
    "# Check if the model exists\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model\n",
    "    model = load(model_path)\n",
    "    print(\"Model loaded from\", model_path)\n",
    "else:\n",
    "    # Instantiate the model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    dump(model, model_path)\n",
    "    print(\"Model trained and saved to\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30048465-335d-4615-b9ec-ad38be77a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.8%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {100*np.mean(model.predict(X_test) == y_test):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535a641-e67e-4b19-9005-25d907c142cf",
   "metadata": {},
   "source": [
    "# Predict on SemEval\n",
    "\n",
    "Now that we've trained a model, we can use it to predict on the original set we were interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2bfde5c-e943-41b8-b900-8b730f28080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_data.loc[:, 'document_vector'] = wsd_data.processed_context.apply(lambda x: [span_to_id.get(span) for span in x if span in span_to_id.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a7ce641-8c8f-4a06-a4e0-5b8d7b6d5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_term_document_matrix = create_term_document_matrix(wsd_data.document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40f4343d-d6dd-4955-8b09-3a67006317dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data - replace with your actual data\n",
    "X = normalize(term_document_matrix, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1ca465e-e900-43af-9ef2-7ee8240d825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([305, 305, 228, ..., 332, 807, 447], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_preds = model.predict(X).astype(np.int32)\n",
    "wsd_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "751ce474-9cea-436b-8b10-9af01fe7ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_lemma_sense_keys = {v: k for k,v in lemma_sense_key_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77d0ddf9-9818-4fd3-89aa-d735ccdd5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_data['semcor_pred'] = pd.Series([id_to_lemma_sense_keys[pred] for pred in wsd_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "100fad34-1833-4936-a846-fee2fb6d4b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>semcor_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{group%1:03:00::}</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{plan%1:09:00::}</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{emission%1:27:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{climate%1:26:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>{copenhagen%1:15:00::}</td>\n",
       "      <td>day%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>{big_league%1:14:00::}</td>\n",
       "      <td>day%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>{vice_president%1:18:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>{policy%1:10:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>{heavyweight%1:18:00::}</td>\n",
       "      <td>election%1:04:01::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        targets         semcor_pred\n",
       "0             {group%1:03:00::}     group%1:03:00::\n",
       "1              {plan%1:09:00::}     group%1:03:00::\n",
       "2          {emission%1:27:00::}  election%1:04:01::\n",
       "3           {climate%1:26:00::}  election%1:04:01::\n",
       "4        {conference%1:14:00::}  election%1:04:01::\n",
       "..                          ...                 ...\n",
       "189      {copenhagen%1:15:00::}       day%1:28:00::\n",
       "190      {big_league%1:14:00::}       day%1:28:00::\n",
       "191  {vice_president%1:18:00::}  election%1:04:01::\n",
       "192          {policy%1:10:00::}  election%1:04:01::\n",
       "193     {heavyweight%1:18:00::}  election%1:04:01::\n",
       "\n",
       "[194 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.loc[:, ['targets', 'semcor_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91bf7dab-f160-4c53-a88a-0ff1f8309a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {100 * wsd_data.apply(lambda x: x.semcor_pred in x.targets, axis=1).mean():.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3165b75-40be-4e5d-9d3e-79d1184f3831",
   "metadata": {},
   "source": [
    "This is disappointingly low. Probably I need to make my own seed set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb45ad-9f23-443d-9538-2c11878f0f8e",
   "metadata": {},
   "source": [
    "## Using Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c48b3bf3-2fa9-4a61-8167-b8027f163b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"/network/weights/llama.var/llama2/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0c4bd92-1fc7-4261-ae68-7a5474e325a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.1 ms, sys: 23.8 ms, total: 82.9 ms\n",
      "Wall time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Set pad_token_id\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c17fa183-d2a9-4af5-98da-e2b9bcfee6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51d4384d559412e9f459f9858c019a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 34.6 s, total: 54.2 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1dcdbcd8-3f0a-4f02-b5db-efcfcc2df443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "Answer: Based on your interest in \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might enjoy:\n",
      "\n",
      "1. \"The Sopranos\" - This HBO series explores the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
      "2. \"The Wire\" - Set in Baltimore, this show delves into the drug trade and the impact it has on the city and its residents. It's known for its gritty realism and complex characters.\n",
      "3. \"Narcos\" - This Netflix series tells the true story of the rise and fall of Colombian drug lord Pablo Escobar and the Medellín cartel.\n",
      "4. \"Sons\n",
      "CPU times: user 5.37 s, sys: 1.06 s, total: 6.43 s\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sequences = text_generation_pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e36d6d4-a945-4144-b882-bd58e4623f0a",
   "metadata": {},
   "source": [
    "## Generating a seed set\n",
    "\n",
    "Ok this works - we can use it to generate our own seed sets for bootstrapping. First we need to collect the lemma sense key labels we are trying to fit for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e667164f-fa94-45de-a228-4ed002c0f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset_from_key(sense_key):\n",
    "    lemma = wn.lemma_from_key(sense_key)\n",
    "    return lemma.synset().name()\n",
    "\n",
    "def get_synset_definition(synset_id):\n",
    "    synset = wn.synset(synset_id)\n",
    "    return synset.definition()\n",
    "\n",
    "def get_synset_examples(synset_id):\n",
    "    synset = wn.synset(synset_id)\n",
    "    return synset.examples()\n",
    "\n",
    "def generate_prompt(word, definition, examples):\n",
    "    prompt = f'Below is one definition for the word/phrase \"{word}\" and some\\n' \\\n",
    "    'example sentences using the same definition.\\n' \\\n",
    "    '\\n' \\\n",
    "    f'Definition ({word}): {definition}'\n",
    "\n",
    "    if examples:\n",
    "        formatted_examples = \"\\n- \".join(examples)\n",
    "        prompt += f'\\nExamples:\\n- {formatted_examples}\\n'\n",
    "\n",
    "    prompt += '\\nPlease generate 5 more example sentences than those provided.'\n",
    "    return prompt\n",
    "\n",
    "def extract_examples(text):\n",
    "    pattern = r\"(\\*|•|-|\\d[\\).])\\s*(.*)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return [match[1] for match in matches]\n",
    "\n",
    "file_path = '../data/lemma_sense_key_data.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    lemma_sense_key_data = pd.read_csv(file_path)\n",
    "    lemma_sense_key_data['examples'] = lemma_sense_key_data['examples'].apply(json.loads)\n",
    "else:\n",
    "    # Code to build the DataFrame from scratch\n",
    "    lemma_sense_keys = sorted(set(key for keys in dev_key.values() for key in keys))\n",
    "    lemma_sense_key_data = pd.DataFrame({'sense_key': lemma_sense_keys})\n",
    "    \n",
    "    lemma_sense_key_data['synset_id'] = lemma_sense_key_data['sense_key'].apply(synset_from_key)\n",
    "    lemma_sense_key_data['word'] = lemma_sense_key_data['synset_id'].str.split('.').apply(lambda x: x[0])\n",
    "    lemma_sense_key_data['definition'] = lemma_sense_key_data['synset_id'].apply(get_synset_definition)\n",
    "    lemma_sense_key_data['examples'] = lemma_sense_key_data['synset_id'].apply(get_synset_examples)\n",
    "    lemma_sense_key_data['prompt'] = lemma_sense_key_data.apply(lambda x: generate_prompt(x['word'], x['definition'], x['examples']), axis=1)\n",
    "\n",
    "    prompts = lemma_sense_key_data['prompt'].tolist()\n",
    "    dataset = Dataset.from_dict({'prompt': prompts})\n",
    "    batch_size = 14  # Adjust as needed\n",
    "    \n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]['prompt']\n",
    "        batch_results = text_generation_pipeline(\n",
    "            batch,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=400\n",
    "        )\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    lemma_sense_key_data['generated_text'] = [r[0]['generated_text'] for r in results]\n",
    "    lemma_sense_key_data.loc[:, 'generated_text'] = lemma_sense_key_data.apply(lambda x: x['generated_text'].replace(x['prompt'], ''), axis=1)\n",
    "    lemma_sense_key_data['generated_examples'] = lemma_sense_key_data['generated_text'].apply(extract_examples)\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    lemma_sense_key_data_to_save = lemma_sense_key_data.copy()\n",
    "    lemma_sense_key_data_to_save['examples'] = lemma_sense_key_data_to_save.examples.apply(json.dumps)\n",
    "    lemma_sense_key_data_to_save['generated_examples'] = lemma_sense_key_data_to_save.generated_examples.apply(json.dumps)\n",
    "    lemma_sense_key_data_to_save.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55312d70-bbd1-49e2-82ae-2fca6a1679f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sense_key</th>\n",
       "      <th>synset_id</th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>examples</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>generated_examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>action%1:04:02::</td>\n",
       "      <td>action.n.01</td>\n",
       "      <td>action</td>\n",
       "      <td>something done (usually as opposed to somethin...</td>\n",
       "      <td>[there were stories of murders and other unnat...</td>\n",
       "      <td>Below is one definition for the word/phrase \"a...</td>\n",
       "      <td>-the company promised to take immediate action...</td>\n",
       "      <td>[\"the company promised to take immediate actio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>action%1:04:04::</td>\n",
       "      <td>action.n.09</td>\n",
       "      <td>action</td>\n",
       "      <td>an act by a government body or supranational o...</td>\n",
       "      <td>[recent federal action undermined the segregat...</td>\n",
       "      <td>Below is one definition for the word/phrase \"a...</td>\n",
       "      <td>\\nYour Turn! Here are five more example senten...</td>\n",
       "      <td>[\"The European Union's action in imposing econ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>activist%1:18:00::</td>\n",
       "      <td>militant.n.01</td>\n",
       "      <td>militant</td>\n",
       "      <td>a militant reformer</td>\n",
       "      <td>[]</td>\n",
       "      <td>Below is one definition for the word/phrase \"m...</td>\n",
       "      <td>is someone who is determined and strong in\\nf...</td>\n",
       "      <td>[\"She is a militant activist who has dedicated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advance%1:11:01::</td>\n",
       "      <td>improvement.n.01</td>\n",
       "      <td>improvement</td>\n",
       "      <td>a change for the better; progress in development</td>\n",
       "      <td>[]</td>\n",
       "      <td>Below is one definition for the word/phrase \"i...</td>\n",
       "      <td>or performance.\\n\\nExample sentences using th...</td>\n",
       "      <td>[\"This new software has made a significant imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adviser%1:18:00::</td>\n",
       "      <td>adviser.n.01</td>\n",
       "      <td>adviser</td>\n",
       "      <td>an expert who gives advice</td>\n",
       "      <td>[an adviser helped students select their cours...</td>\n",
       "      <td>Below is one definition for the word/phrase \"a...</td>\n",
       "      <td>- a financial adviser counseled clients on inv...</td>\n",
       "      <td>[\"a financial adviser counseled clients on inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>week%1:28:02::</td>\n",
       "      <td>week.n.03</td>\n",
       "      <td>week</td>\n",
       "      <td>a period of seven consecutive days starting on...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Below is one definition for the word/phrase \"w...</td>\n",
       "      <td>and ending on Saturday\\n\\nExample sentences:\\...</td>\n",
       "      <td>[\"I will take the next week off work to go on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>working_group%1:14:00::</td>\n",
       "      <td>working_group.n.01</td>\n",
       "      <td>working_group</td>\n",
       "      <td>a group of people working together temporarily...</td>\n",
       "      <td>[the working group was supposed to report back...</td>\n",
       "      <td>Below is one definition for the word/phrase \"w...</td>\n",
       "      <td>- the working group included representatives f...</td>\n",
       "      <td>[\"the working group included representatives f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>world%1:05:00::</td>\n",
       "      <td>world.n.08</td>\n",
       "      <td>world</td>\n",
       "      <td>all of the living human inhabitants of the earth</td>\n",
       "      <td>[all the world loves a lover, she always used ...</td>\n",
       "      <td>Below is one definition for the word/phrase \"w...</td>\n",
       "      <td>- he has no concern for the world beyond his o...</td>\n",
       "      <td>[\"he has no concern for the world beyond his o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>world%1:17:00::</td>\n",
       "      <td>earth.n.01</td>\n",
       "      <td>earth</td>\n",
       "      <td>the 3rd planet from the sun; the planet we liv...</td>\n",
       "      <td>[the Earth moves around the sun, he sailed aro...</td>\n",
       "      <td>Below is one definition for the word/phrase \"e...</td>\n",
       "      <td>- the Earth is blue\\n- she studied geology on ...</td>\n",
       "      <td>[\"the Earth is blue\", \"she studied geology on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>year%1:28:01::</td>\n",
       "      <td>year.n.01</td>\n",
       "      <td>year</td>\n",
       "      <td>a period of time containing 365 (or 366) days</td>\n",
       "      <td>[she is 4 years old, in the year 1920]</td>\n",
       "      <td>Below is one definition for the word/phrase \"y...</td>\n",
       "      <td>- the company has been in business for 10 year...</td>\n",
       "      <td>[\"the company has been in business for 10 year...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sense_key           synset_id           word  \\\n",
       "0           action%1:04:02::         action.n.01         action   \n",
       "1           action%1:04:04::         action.n.09         action   \n",
       "2         activist%1:18:00::       militant.n.01       militant   \n",
       "3          advance%1:11:01::    improvement.n.01    improvement   \n",
       "4          adviser%1:18:00::        adviser.n.01        adviser   \n",
       "..                       ...                 ...            ...   \n",
       "126           week%1:28:02::           week.n.03           week   \n",
       "127  working_group%1:14:00::  working_group.n.01  working_group   \n",
       "128          world%1:05:00::          world.n.08          world   \n",
       "129          world%1:17:00::          earth.n.01          earth   \n",
       "130           year%1:28:01::           year.n.01           year   \n",
       "\n",
       "                                            definition  \\\n",
       "0    something done (usually as opposed to somethin...   \n",
       "1    an act by a government body or supranational o...   \n",
       "2                                  a militant reformer   \n",
       "3     a change for the better; progress in development   \n",
       "4                           an expert who gives advice   \n",
       "..                                                 ...   \n",
       "126  a period of seven consecutive days starting on...   \n",
       "127  a group of people working together temporarily...   \n",
       "128   all of the living human inhabitants of the earth   \n",
       "129  the 3rd planet from the sun; the planet we liv...   \n",
       "130      a period of time containing 365 (or 366) days   \n",
       "\n",
       "                                              examples  \\\n",
       "0    [there were stories of murders and other unnat...   \n",
       "1    [recent federal action undermined the segregat...   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4    [an adviser helped students select their cours...   \n",
       "..                                                 ...   \n",
       "126                                                 []   \n",
       "127  [the working group was supposed to report back...   \n",
       "128  [all the world loves a lover, she always used ...   \n",
       "129  [the Earth moves around the sun, he sailed aro...   \n",
       "130             [she is 4 years old, in the year 1920]   \n",
       "\n",
       "                                                prompt  \\\n",
       "0    Below is one definition for the word/phrase \"a...   \n",
       "1    Below is one definition for the word/phrase \"a...   \n",
       "2    Below is one definition for the word/phrase \"m...   \n",
       "3    Below is one definition for the word/phrase \"i...   \n",
       "4    Below is one definition for the word/phrase \"a...   \n",
       "..                                                 ...   \n",
       "126  Below is one definition for the word/phrase \"w...   \n",
       "127  Below is one definition for the word/phrase \"w...   \n",
       "128  Below is one definition for the word/phrase \"w...   \n",
       "129  Below is one definition for the word/phrase \"e...   \n",
       "130  Below is one definition for the word/phrase \"y...   \n",
       "\n",
       "                                        generated_text  \\\n",
       "0    -the company promised to take immediate action...   \n",
       "1    \\nYour Turn! Here are five more example senten...   \n",
       "2     is someone who is determined and strong in\\nf...   \n",
       "3     or performance.\\n\\nExample sentences using th...   \n",
       "4    - a financial adviser counseled clients on inv...   \n",
       "..                                                 ...   \n",
       "126   and ending on Saturday\\n\\nExample sentences:\\...   \n",
       "127  - the working group included representatives f...   \n",
       "128  - he has no concern for the world beyond his o...   \n",
       "129  - the Earth is blue\\n- she studied geology on ...   \n",
       "130  - the company has been in business for 10 year...   \n",
       "\n",
       "                                    generated_examples  \n",
       "0    [\"the company promised to take immediate actio...  \n",
       "1    [\"The European Union's action in imposing econ...  \n",
       "2    [\"She is a militant activist who has dedicated...  \n",
       "3    [\"This new software has made a significant imp...  \n",
       "4    [\"a financial adviser counseled clients on inv...  \n",
       "..                                                 ...  \n",
       "126  [\"I will take the next week off work to go on ...  \n",
       "127  [\"the working group included representatives f...  \n",
       "128  [\"he has no concern for the world beyond his o...  \n",
       "129  [\"the Earth is blue\", \"she studied geology on ...  \n",
       "130  [\"the company has been in business for 10 year...  \n",
       "\n",
       "[131 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sense_key_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
