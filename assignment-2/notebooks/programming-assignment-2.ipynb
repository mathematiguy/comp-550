{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878b0ab7-c6b0-4db4-9e7f-2c0437b50984",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "24a91c0b-d23e-4ba8-a92a-c729517f9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from functools import reduce\n",
    "\n",
    "from loader import *\n",
    "\n",
    "from nltk import wsd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from collections import Counter\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9c418-31dd-4b89-a970-31e116bea65a",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d81e2a-bc47-4170-91d0-f04463b25fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "1450\n"
     ]
    }
   ],
   "source": [
    "data_f = \"../code/multilingual-all-words.en.xml\"\n",
    "key_f = \"../code/wordnet.en.key\"\n",
    "dev_instances, test_instances = load_instances(data_f)\n",
    "dev_key, test_key = load_key(key_f)\n",
    "\n",
    "# IMPORTANT: keys contain fewer entries than the instances; need to remove them\n",
    "dev_instances = {k: v for (k, v) in dev_instances.items() if k in dev_key}\n",
    "test_instances = {k: v for (k, v) in test_instances.items() if k in test_key}\n",
    "\n",
    "# ready to use here\n",
    "print(len(dev_instances))  # number of dev instances\n",
    "print(len(test_instances))  # number of test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff380b56-d627-44e2-93ee-03b9a94bf4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'id', 'index', 'lemma']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(dev_instances[\"d001.s001.t002\"]) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602b304d-1860-4d5a-81cd-7463db2e9127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group%1:03:00::']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_key['d001.s001.t002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b73454b-d868-4664-aa0a-07dc61182f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001.s001.t002\tgroup\tU.N. group draft plan to reduce emission\t1\n"
     ]
    }
   ],
   "source": [
    "print(dev_instances[\"d001.s001.t002\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f1b7fc-9645-4b0f-b696-69e3f8f9111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dev_instances[\"d001.s001.t002\"].context\n",
    "id = dev_instances[\"d001.s001.t002\"].id\n",
    "index = dev_instances[\"d001.s001.t002\"].index\n",
    "lemma = dev_instances[\"d001.s001.t002\"].lemma\n",
    "\n",
    "assert lemma == context[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b28efa-d4f9-4427-aaee-1491943c3e3e",
   "metadata": {},
   "source": [
    "## Look at synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaaf5d1-e8da-4c8e-a5fd-5f02ed06f05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets('dog')\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fadd2121-6809-4326-b1fc-29975f4fd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wn.synsets(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d109b4-6c24-46f9-87a4-8c5f51882adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: group.n.01\n",
      "Lemmas: ['group', 'grouping']\n",
      "Definition: any number of entities (members) considered as a unit\n",
      "Examples: []\n",
      "\n",
      "Synset: group.n.02\n",
      "Lemmas: ['group', 'radical', 'chemical_group']\n",
      "Definition: (chemistry) two or more atoms bound together as a single unit and forming part of a molecule\n",
      "Examples: []\n",
      "\n",
      "Synset: group.n.03\n",
      "Lemmas: ['group', 'mathematical_group']\n",
      "Definition: a set that is closed, associative, has an identity element and every element has an inverse\n",
      "Examples: []\n",
      "\n",
      "Synset: group.v.01\n",
      "Lemmas: ['group']\n",
      "Definition: arrange into a group or groups\n",
      "Examples: ['Can you group these shapes together?']\n",
      "\n",
      "Synset: group.v.02\n",
      "Lemmas: ['group', 'aggroup']\n",
      "Definition: form a group or group together\n",
      "Examples: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for syn in synsets:\n",
    "    print(\"Synset:\", syn.name())\n",
    "    print(\"Lemmas:\", [lemma.name() for lemma in syn.lemmas()])\n",
    "    print(\"Definition:\", syn.definition())\n",
    "    print(\"Examples:\", syn.examples())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4fb26-b8a3-42bd-8ec9-b9054c2d512c",
   "metadata": {},
   "source": [
    "## Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd79eef-7d35-4800-a058-8bb59bdc2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, stop_words=stop_words, lemmatizer=lemmatizer):\n",
    "    \"\"\"\n",
    "    Preprocesses the sentence by lemmatizing and removing stop words and words without any alphanumeric characters.\n",
    "    Assumes sentence is a list of tokenized words.\n",
    "    \"\"\"\n",
    "    # Helper function to check for at least one alphanumeric character in a word\n",
    "    contains_alnum = lambda word: any(char.isalnum() for char in word)\n",
    "\n",
    "    # Lemmatize words, filter out stop words and words without any alphanumeric characters\n",
    "    processed = {lemmatizer.lemmatize(w) for w in sentence if w not in stop_words and contains_alnum(w)}\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "def lesk(lemma, context):\n",
    "    \"\"\"\n",
    "    Lesk's algorithm implementation.\n",
    "    Assumes preprocessed_context is a set of lemmatized words without stop words.\n",
    "    \"\"\"\n",
    "    assert isinstance(lemma, str), \"Lemma is not a string\"\n",
    "    assert len(context) > 0, \"Empty context\"\n",
    "\n",
    "    max_overlap = 0\n",
    "    best_sense = None\n",
    "\n",
    "    context = preprocess(context)\n",
    "\n",
    "    # Obtain the synsets for the lemma\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    # Default to the most common sense if synsets are available\n",
    "    if synsets:\n",
    "        best_sense = synsets[0]\n",
    "\n",
    "    for sense in synsets:\n",
    "        # Preprocess the signature (definition and examples)\n",
    "        signature = preprocess(word_tokenize(sense.definition()), stop_words, lemmatizer)\n",
    "        for example in sense.examples():\n",
    "            signature |= preprocess(word_tokenize(example), stop_words, lemmatizer)\n",
    "\n",
    "        # The overlap is the size of the intersection\n",
    "        overlap = len(context & signature)\n",
    "\n",
    "        # Keep track of the best overlap so far\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f6d81c-4c98-4b27-a485-29ef938a92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = 'climate'\n",
    "context = 'the U.N.-sponsored climate conference -- characterize so far by unruly posturing and mutual recrimination -- gain renewed focus Friday with the release of a document outline ambitious greenhouse-gas reduction over the next @card@ year , with industrialized_nation shoulder most of the burden in the near term .'.split()\n",
    "best_sense = lesk(lemma, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9cb539d-a7eb-45ab-84ea-20b2469cd1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climate%1:26:00::', 'clime%1:26:00::']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_keys = [lemma.key() for lemma in best_sense.lemmas()]\n",
    "sense_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f824b983-f7c2-45df-ae59-32d06d5bfe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001.s001.t002\tgroup\tU.N. group draft plan to reduce emission\t1\n",
      "arrange into a group or groups\n",
      "\n",
      "d001.s001.t003\tplan\tU.N. group draft plan to reduce emission\t3\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "d001.s001.t004\temission\tU.N. group draft plan to reduce emission\t6\n",
      "the act of emitting; causing to flow forth\n",
      "\n",
      "d001.s002.t001\tclimate\tthe U.N.-sponsored climate conference -- characterize so far by unruly posturing and mutual recrimination -- gain renewed focus Friday with the release of a document outline ambitious greenhouse-gas reduction over the next @card@ year , with industrialized_nation shoulder most of the burden in the near term .\t2\n",
      "the weather in some location averaged over some long period of time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(dev_instances.items()):\n",
    "    key, instance = item\n",
    "    lemma = instance.lemma\n",
    "    context = instance.context\n",
    "    print(instance)\n",
    "    print(lesk(lemma, context).definition())\n",
    "    print()\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69954ef1-5b65-449b-979d-7208f0df43c6",
   "metadata": {},
   "source": [
    "## Lemma sense keys and synset numbers\n",
    "\n",
    "The correspondence between lemma sense keys and synset numbers is stored in `wordnet.en.key`. We can just read from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b251ce6-df0a-4d7e-84cd-6cce1afcbfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001 d001.s001.t002 group%1:03:00:: \n",
      "d001 d001.s001.t003 plan%1:09:00:: \n",
      "d001 d001.s001.t004 emission%1:27:00:: \n",
      "d001 d001.s002.t001 climate%1:26:00:: \n",
      "d001 d001.s002.t002 conference%1:14:00:: \n",
      "d001 d001.s002.t003 posturing%1:07:00:: \n",
      "d001 d001.s002.t004 recrimination%1:10:00:: \n",
      "d001 d001.s002.t005 focus%1:09:00:: \n",
      "d001 d001.s002.t006 friday%1:28:00:: \n",
      "d001 d001.s002.t007 release%1:22:00:: \n"
     ]
    }
   ],
   "source": [
    "! head ../code/wordnet.en.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a09341-be3d-47d8-9e9c-16c2487e6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_sense_key_to_synset_number_correspondence():\n",
    "    wordnet_key_file = '../code/wordnet.en.key'\n",
    "    lsk_to_sn = {}\n",
    "    with open(wordnet_key_file, 'r') as f:\n",
    "        for line in f.read().strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            _, lsk, sn = line.split(' ', 2)\n",
    "            lsk_to_sn[lsk] = set(sn.split(' '))\n",
    "    return lsk_to_sn\n",
    "\n",
    "lsk_to_sn = get_lemma_sense_key_to_synset_number_correspondence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f93881-f3e9-4017-98f2-4cddb5f67a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d001.s001.t002': {'group%1:03:00::'},\n",
       " 'd001.s001.t003': {'plan%1:09:00::'},\n",
       " 'd001.s001.t004': {'emission%1:27:00::'},\n",
       " 'd001.s002.t001': {'climate%1:26:00::'},\n",
       " 'd001.s002.t002': {'conference%1:14:00::'},\n",
       " 'd001.s002.t003': {'posturing%1:07:00::'},\n",
       " 'd001.s002.t004': {'recrimination%1:10:00::'},\n",
       " 'd001.s002.t005': {'focus%1:09:00::'},\n",
       " 'd001.s002.t006': {'friday%1:28:00::'},\n",
       " 'd001.s002.t007': {'release%1:22:00::'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top ten\n",
    "{x[0]: x[1] for i, x in enumerate(lsk_to_sn.items()) if i < 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cb7f5-901e-46fb-98f6-4e263506cdcf",
   "metadata": {},
   "source": [
    "## Calculating accuracy\n",
    "\n",
    "Now that I have this correspondence, I can calculate the accuracy of my lesk's algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1295f176-5ab8-4981-96b1-d8f926555efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wsd_data():\n",
    "    wsd_data = []\n",
    "    for key, instance in dev_instances.items():\n",
    "        id = instance.id\n",
    "        lemma = instance.lemma\n",
    "        context = instance.context\n",
    "    \n",
    "        processed_context = preprocess(context)\n",
    "    \n",
    "        # Use lesk's algorithm to guess the synset\n",
    "        synset = lesk(lemma, context)\n",
    "    \n",
    "        # Get the sense-keys for the predicted synset\n",
    "        preds = set(lemma.key() for lemma in synset.lemmas())\n",
    "        \n",
    "        # Extract the synset number from the sense-key\n",
    "        targets = lsk_to_sn[id]\n",
    "        \n",
    "        # Calculate if there is any overlap between the predicted sense and the target\n",
    "        match = len(preds & targets) > 0\n",
    "        \n",
    "        wsd_data.append(\n",
    "            dict(id=id, lemma=lemma, context=context, processed_context=processed_context, synset=synset, preds=preds, targets=targets, match=match)\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(wsd_data)\n",
    "\n",
    "wsd_data = build_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6456356c-bf2f-4903-b145-f5a43ce62470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d001.s001.t002</td>\n",
       "      <td>group</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('group.v.01')</td>\n",
       "      <td>{group%2:31:00::}</td>\n",
       "      <td>{group%1:03:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d001.s001.t003</td>\n",
       "      <td>plan</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('plan.n.01')</td>\n",
       "      <td>{plan%1:09:00::, program%1:09:00::, programme%...</td>\n",
       "      <td>{plan%1:09:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d001.s001.t004</td>\n",
       "      <td>emission</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('emission.n.01')</td>\n",
       "      <td>{emission%1:04:00::, emanation%1:04:00::}</td>\n",
       "      <td>{emission%1:27:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d001.s002.t001</td>\n",
       "      <td>climate</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{far, focus, renewed, gain, ambitious, documen...</td>\n",
       "      <td>Synset('climate.n.01')</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d001.s002.t002</td>\n",
       "      <td>conference</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{far, focus, renewed, gain, ambitious, documen...</td>\n",
       "      <td>Synset('conference.n.01')</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       lemma  \\\n",
       "0  d001.s001.t002       group   \n",
       "1  d001.s001.t003        plan   \n",
       "2  d001.s001.t004    emission   \n",
       "3  d001.s002.t001     climate   \n",
       "4  d001.s002.t002  conference   \n",
       "\n",
       "                                             context  \\\n",
       "0   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "1   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "2   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "3  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "4  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "\n",
       "                                   processed_context  \\\n",
       "0       {draft, plan, group, emission, reduce, U.N.}   \n",
       "1       {draft, plan, group, emission, reduce, U.N.}   \n",
       "2       {draft, plan, group, emission, reduce, U.N.}   \n",
       "3  {far, focus, renewed, gain, ambitious, documen...   \n",
       "4  {far, focus, renewed, gain, ambitious, documen...   \n",
       "\n",
       "                      synset  \\\n",
       "0       Synset('group.v.01')   \n",
       "1        Synset('plan.n.01')   \n",
       "2    Synset('emission.n.01')   \n",
       "3     Synset('climate.n.01')   \n",
       "4  Synset('conference.n.01')   \n",
       "\n",
       "                                               preds                 targets  \\\n",
       "0                                  {group%2:31:00::}       {group%1:03:00::}   \n",
       "1  {plan%1:09:00::, program%1:09:00::, programme%...        {plan%1:09:00::}   \n",
       "2          {emission%1:04:00::, emanation%1:04:00::}    {emission%1:27:00::}   \n",
       "3               {climate%1:26:00::, clime%1:26:00::}     {climate%1:26:00::}   \n",
       "4                             {conference%1:14:00::}  {conference%1:14:00::}   \n",
       "\n",
       "   match  \n",
       "0  False  \n",
       "1   True  \n",
       "2  False  \n",
       "3   True  \n",
       "4   True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c92099ed-2ff9-464b-958c-75e2484b1ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      d001.s001.t003\n",
       "lemma                                                             plan\n",
       "context               [U.N., group, draft, plan, to, reduce, emission]\n",
       "processed_context         {draft, plan, group, emission, reduce, U.N.}\n",
       "synset                                             Synset('plan.n.01')\n",
       "preds                {plan%1:09:00::, program%1:09:00::, programme%...\n",
       "targets                                               {plan%1:09:00::}\n",
       "match                                                             True\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54905c5b-1bd7-4546-ae73-6f044c001bad",
   "metadata": {},
   "source": [
    "I'm checking that the preprocessing step correctly resolves multi-word phrases to single entities and it appears it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca1b206-abb4-49e3-9567-da1997f533c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>d001.s016.t001</td>\n",
       "      <td>comment</td>\n",
       "      <td>[Stern, make, his, comment, an, hour, after, C...</td>\n",
       "      <td>{united_states, Chinese, negotiator, week, say...</td>\n",
       "      <td>Synset('comment.v.01')</td>\n",
       "      <td>{point_out%2:32:01::, remark%2:32:01::, commen...</td>\n",
       "      <td>{comment%1:10:00::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>d001.s016.t002</td>\n",
       "      <td>hour</td>\n",
       "      <td>[Stern, make, his, comment, an, hour, after, C...</td>\n",
       "      <td>{united_states, Chinese, negotiator, week, say...</td>\n",
       "      <td>Synset('hour.n.01')</td>\n",
       "      <td>{60_minutes%1:28:00::, hour%1:28:00::, hr%1:28...</td>\n",
       "      <td>{hour%1:28:01::}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id    lemma  \\\n",
       "99   d001.s016.t001  comment   \n",
       "100  d001.s016.t002     hour   \n",
       "\n",
       "                                               context  \\\n",
       "99   [Stern, make, his, comment, an, hour, after, C...   \n",
       "100  [Stern, make, his, comment, an, hour, after, C...   \n",
       "\n",
       "                                     processed_context  \\\n",
       "99   {united_states, Chinese, negotiator, week, say...   \n",
       "100  {united_states, Chinese, negotiator, week, say...   \n",
       "\n",
       "                     synset  \\\n",
       "99   Synset('comment.v.01')   \n",
       "100     Synset('hour.n.01')   \n",
       "\n",
       "                                                 preds              targets  \\\n",
       "99   {point_out%2:32:01::, remark%2:32:01::, commen...  {comment%1:10:00::}   \n",
       "100  {60_minutes%1:28:00::, hour%1:28:00::, hr%1:28...     {hour%1:28:01::}   \n",
       "\n",
       "     match  \n",
       "99   False  \n",
       "100  False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data[wsd_data['processed_context'].apply(lambda x: 'America' in x)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e5d8753-31b7-4d1c-ad2a-3cd840b5d76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Stern', 'make', 'his', 'comment', 'an', 'hour', 'after', 'Chinese', 'vice', 'foreign_minister', 'he_yafei', 'say', 'America', \"'s\", 'top', 'climate', 'negotiator', 'be', 'either', 'lack', '``', 'common_sense', \"''\", 'or', 'be', '``', 'extremely', 'irresponsible', \"''\", 'for', 'say', 'earlier', 'in', 'the', 'week', 'that', 'the', 'united_states', 'would', 'not', 'help', 'China', 'financially', 'to', 'cope', 'with', 'global_warming', '.']),\n",
       "       {'united_states', 'Chinese', 'negotiator', 'week', 'say', 'top', 'China', 'lack', 'make', 'hour', 'help', 'earlier', 'Stern', 'financially', 'comment', 'irresponsible', 'vice', 'either', 'cope', 'global_warming', \"'s\", 'America', 'climate', 'foreign_minister', 'common_sense', 'would', 'he_yafei', 'extremely'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.loc[101, ['context', 'processed_context']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f237a7c-b8f7-4f77-845a-737474c7cafa",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dbbb464-9d8c-4097-aa92-fe9b267884f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk's algorithm Accuracy: 57.2%\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk\\'s algorithm Accuracy: {100*wsd_data.match.mean():.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11557122-da15-4a2f-8b70-3a31d3416a6a",
   "metadata": {},
   "source": [
    "This seems like a decent score for this task considering the effort we put in.\n",
    "\n",
    "Next we should calculate scores for:\n",
    "- The most frequent sense baseline\n",
    "- NLTK's implementation of Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8643bac-1260-4fb4-b177-79751ece525a",
   "metadata": {},
   "source": [
    "## Most frequent sense baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f55a109a-55be-46fc-9846-b429a8e789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_synset(lemma):\n",
    "    '''\n",
    "    Given a lemma, this returns the most frequent sense for that lemma.\n",
    "    '''\n",
    "    return set(lemma.key() for lemma in wn.synsets(lemma)[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1af565e-19d5-4c42-acae-40d664633ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_data['most_frequent_synset'] = wsd_data.lemma.apply(most_frequent_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279da987-25a9-4c0d-8c68-d2f1ca388065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Synset Accuracy: 67.5%\n"
     ]
    }
   ],
   "source": [
    "most_frequent_synset_accuracy = wsd_data.apply(lambda x: len(x.most_frequent_synset & x.targets) > 0, axis=1).mean()\n",
    "print(f'Most Frequent Synset Accuracy: {100*most_frequent_synset_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de6259-5c0c-40b9-ad22-90a4485d0e9e",
   "metadata": {},
   "source": [
    "## NLTK Lesk Algorithm baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f31c7b2-3860-4308-8f76-3f372fc54f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lesk's Algorithm Accuracy: 34.0%\n"
     ]
    }
   ],
   "source": [
    "wsd_data['nltk_pred_synset'] = wsd_data.apply(lambda x: set(lemma.key() for lemma in wsd.lesk(x.context, x.lemma).lemmas()), axis = 1)\n",
    "nltk_pred_synset_accuracy = wsd_data.apply(lambda x: len(x.nltk_pred_synset & x.targets) > 0, axis=1).mean()\n",
    "print(f'NLTK Lesk\\'s Algorithm Accuracy: {100*nltk_pred_synset_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96d6c8c3-eccd-4652-b0c5-bc6cf9a9fb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>processed_context</th>\n",
       "      <th>synset</th>\n",
       "      <th>preds</th>\n",
       "      <th>targets</th>\n",
       "      <th>match</th>\n",
       "      <th>most_frequent_synset</th>\n",
       "      <th>nltk_pred_synset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d001.s001.t002</td>\n",
       "      <td>group</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('group.v.01')</td>\n",
       "      <td>{group%2:31:00::}</td>\n",
       "      <td>{group%1:03:00::}</td>\n",
       "      <td>False</td>\n",
       "      <td>{group%1:03:00::, grouping%1:03:00::}</td>\n",
       "      <td>{group%2:33:00::, aggroup%2:33:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d001.s001.t003</td>\n",
       "      <td>plan</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('plan.n.01')</td>\n",
       "      <td>{plan%1:09:00::, program%1:09:00::, programme%...</td>\n",
       "      <td>{plan%1:09:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{plan%1:09:00::, program%1:09:00::, programme%...</td>\n",
       "      <td>{plan%2:36:00::, project%2:36:01::, design%2:3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d001.s001.t004</td>\n",
       "      <td>emission</td>\n",
       "      <td>[U.N., group, draft, plan, to, reduce, emission]</td>\n",
       "      <td>{draft, plan, group, emission, reduce, U.N.}</td>\n",
       "      <td>Synset('emission.n.01')</td>\n",
       "      <td>{emission%1:04:00::, emanation%1:04:00::}</td>\n",
       "      <td>{emission%1:27:00::}</td>\n",
       "      <td>False</td>\n",
       "      <td>{emission%1:04:00::, emanation%1:04:00::}</td>\n",
       "      <td>{emission%1:04:00::, emanation%1:04:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d001.s002.t001</td>\n",
       "      <td>climate</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{far, focus, renewed, gain, ambitious, documen...</td>\n",
       "      <td>Synset('climate.n.01')</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "      <td>{climate%1:26:00::, clime%1:26:00::}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d001.s002.t002</td>\n",
       "      <td>conference</td>\n",
       "      <td>[the, U.N.-sponsored, climate, conference, --,...</td>\n",
       "      <td>{far, focus, renewed, gain, ambitious, documen...</td>\n",
       "      <td>Synset('conference.n.01')</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>True</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "      <td>{conference%1:14:00::}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       lemma  \\\n",
       "0  d001.s001.t002       group   \n",
       "1  d001.s001.t003        plan   \n",
       "2  d001.s001.t004    emission   \n",
       "3  d001.s002.t001     climate   \n",
       "4  d001.s002.t002  conference   \n",
       "\n",
       "                                             context  \\\n",
       "0   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "1   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "2   [U.N., group, draft, plan, to, reduce, emission]   \n",
       "3  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "4  [the, U.N.-sponsored, climate, conference, --,...   \n",
       "\n",
       "                                   processed_context  \\\n",
       "0       {draft, plan, group, emission, reduce, U.N.}   \n",
       "1       {draft, plan, group, emission, reduce, U.N.}   \n",
       "2       {draft, plan, group, emission, reduce, U.N.}   \n",
       "3  {far, focus, renewed, gain, ambitious, documen...   \n",
       "4  {far, focus, renewed, gain, ambitious, documen...   \n",
       "\n",
       "                      synset  \\\n",
       "0       Synset('group.v.01')   \n",
       "1        Synset('plan.n.01')   \n",
       "2    Synset('emission.n.01')   \n",
       "3     Synset('climate.n.01')   \n",
       "4  Synset('conference.n.01')   \n",
       "\n",
       "                                               preds                 targets  \\\n",
       "0                                  {group%2:31:00::}       {group%1:03:00::}   \n",
       "1  {plan%1:09:00::, program%1:09:00::, programme%...        {plan%1:09:00::}   \n",
       "2          {emission%1:04:00::, emanation%1:04:00::}    {emission%1:27:00::}   \n",
       "3               {climate%1:26:00::, clime%1:26:00::}     {climate%1:26:00::}   \n",
       "4                             {conference%1:14:00::}  {conference%1:14:00::}   \n",
       "\n",
       "   match                               most_frequent_synset  \\\n",
       "0  False              {group%1:03:00::, grouping%1:03:00::}   \n",
       "1   True  {plan%1:09:00::, program%1:09:00::, programme%...   \n",
       "2  False          {emission%1:04:00::, emanation%1:04:00::}   \n",
       "3   True               {climate%1:26:00::, clime%1:26:00::}   \n",
       "4   True                             {conference%1:14:00::}   \n",
       "\n",
       "                                    nltk_pred_synset  \n",
       "0               {group%2:33:00::, aggroup%2:33:00::}  \n",
       "1  {plan%2:36:00::, project%2:36:01::, design%2:3...  \n",
       "2          {emission%1:04:00::, emanation%1:04:00::}  \n",
       "3               {climate%1:26:00::, clime%1:26:00::}  \n",
       "4                             {conference%1:14:00::}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5652b42-0a62-42b5-9b97-fb6339a9fb07",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Now we'll use the SemCor corpus to apply Yarowsky's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "55ba5512-6f61-43ac-b1a2-82743a55aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_semcor_data():\n",
    "    '''\n",
    "    Uses the semcor corpus to assemble a dataset for bootstrap classification\n",
    "    '''\n",
    "    semcor_data = []\n",
    "    for sent_id, tagged_sent in enumerate(semcor.tagged_sents(tag='sense')):\n",
    "        sentence = [\"_\".join(tree.leaves()) for tree in tagged_sent]\n",
    "        for i, tree in enumerate(tagged_sent):\n",
    "            d = {}\n",
    "            d['sentence'] = sentence\n",
    "            d['sentence_text'] = ' '.join(sentence)\n",
    "            d['processed_sentence'] = preprocess(sentence)\n",
    "            d['span'] = sentence[i]\n",
    "    \n",
    "            if hasattr(tree, 'label') and isinstance(tree.label(), Lemma):\n",
    "                d['sense_key'] = tree.label().key()\n",
    "            else:\n",
    "                continue\n",
    "            semcor_data.append(d)\n",
    "    \n",
    "    return pd.DataFrame(semcor_data)\n",
    "\n",
    "semcor_data = build_semcor_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a22f4a19-b51f-4207-9ca3-f6f8bda9a47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>said</td>\n",
       "      <td>say%2:32:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>investigation%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>atlanta%1:15:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224711</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>stung</td>\n",
       "      <td>sting%2:39:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224712</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>was</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224713</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>let</td>\n",
       "      <td>let%2:41:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224714</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>make</td>\n",
       "      <td>make%2:41:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224715</th>\n",
       "      <td>[``, I, can, n't, turn, the, studio, into, a, ...</td>\n",
       "      <td>`` I can n't turn the studio into a gambling_h...</td>\n",
       "      <td>{n't, I, said, gambling_hell, studio, saloon, ...</td>\n",
       "      <td>said</td>\n",
       "      <td>say%2:32:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "224711  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224712  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224713  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224714  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224715  [``, I, can, n't, turn, the, studio, into, a, ...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "224711  Her reply stung me , but this was too importan...   \n",
       "224712  Her reply stung me , but this was too importan...   \n",
       "224713  Her reply stung me , but this was too importan...   \n",
       "224714  Her reply stung me , but this was too importan...   \n",
       "224715  `` I can n't turn the studio into a gambling_h...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "1       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "2       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "3       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "4       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "...                                                   ...   \n",
       "224711  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224712  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224713  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224714  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224715  {n't, I, said, gambling_hell, studio, saloon, ...   \n",
       "\n",
       "                            span                sense_key  \n",
       "0       Fulton_County_Grand_Jury          group%1:03:00::  \n",
       "1                           said            say%2:32:00::  \n",
       "2                         Friday         friday%1:28:00::  \n",
       "3                  investigation  investigation%1:09:00::  \n",
       "4                        Atlanta        atlanta%1:15:00::  \n",
       "...                          ...                      ...  \n",
       "224711                     stung          sting%2:39:02::  \n",
       "224712                       was             be%2:42:03::  \n",
       "224713                       let            let%2:41:00::  \n",
       "224714                      make           make%2:41:00::  \n",
       "224715                      said            say%2:32:00::  \n",
       "\n",
       "[224716 rows x 5 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d60ac8-171f-46ca-a88e-e810230c1d75",
   "metadata": {},
   "source": [
    "## Blank out lemma sense keys\n",
    "\n",
    "We need to replace the lemma sense keys that are not in our dev/test set with None. We keep the processed sentences, so that when we form term-document matrices we still have all of the tokens but we will only predict cases where the sense_key is valid and in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bab4f246-25e9-4d8a-a8d8-6cf39e803182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224711</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>stung</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224712</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>was</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224713</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>let</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224714</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>make</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224715</th>\n",
       "      <td>[``, I, can, n't, turn, the, studio, into, a, ...</td>\n",
       "      <td>`` I can n't turn the studio into a gambling_h...</td>\n",
       "      <td>{n't, I, said, gambling_hell, studio, saloon, ...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "224711  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224712  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224713  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224714  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224715  [``, I, can, n't, turn, the, studio, into, a, ...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "224711  Her reply stung me , but this was too importan...   \n",
       "224712  Her reply stung me , but this was too importan...   \n",
       "224713  Her reply stung me , but this was too importan...   \n",
       "224714  Her reply stung me , but this was too importan...   \n",
       "224715  `` I can n't turn the studio into a gambling_h...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "1       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "2       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "3       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "4       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "...                                                   ...   \n",
       "224711  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224712  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224713  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224714  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224715  {n't, I, said, gambling_hell, studio, saloon, ...   \n",
       "\n",
       "                            span         sense_key  \n",
       "0       Fulton_County_Grand_Jury   group%1:03:00::  \n",
       "1                           said              None  \n",
       "2                         Friday  friday%1:28:00::  \n",
       "3                  investigation              None  \n",
       "4                        Atlanta              None  \n",
       "...                          ...               ...  \n",
       "224711                     stung              None  \n",
       "224712                       was              None  \n",
       "224713                       let              None  \n",
       "224714                      make              None  \n",
       "224715                      said              None  \n",
       "\n",
       "[224716 rows x 5 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sense_keys = set([k for v in list(dev_key.values()) + list(test_key.values()) for k in v])\n",
    "semcor_data['sense_key'] = semcor_data.sense_key.apply(lambda x: x if x in lemma_sense_keys else None)\n",
    "semcor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3847c736-6149-42d5-ab23-e40886198b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 14,114 valid sentence-sense_key pairs in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {(~semcor_data.sense_key.isnull()).sum():,d} valid sentence-sense_key pairs in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0a7bb-5871-4860-bada-0741922cc351",
   "metadata": {},
   "source": [
    "## Calculate lemma sense ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fd914951-ca9d-4742-bcc3-35f161dc56f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4  [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4  The Fulton_County_Grand_Jury said Friday an in...   \n",
       "\n",
       "                                  processed_sentence  \\\n",
       "0  {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "1  {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "2  {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "3  {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "4  {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "\n",
       "                       span         sense_key  sense_id  \n",
       "0  Fulton_County_Grand_Jury   group%1:03:00::     305.0  \n",
       "1                      said              None       NaN  \n",
       "2                    Friday  friday%1:28:00::     288.0  \n",
       "3             investigation              None       NaN  \n",
       "4                   Atlanta              None       NaN  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sense_keys = sorted(lemma_sense_keys)\n",
    "lemma_sense_key_to_id = {v: k for k, v in enumerate(lemma_sense_keys)}\n",
    "\n",
    "semcor_data['sense_id'] = semcor_data.sense_key.apply(lambda x: lemma_sense_key_to_id[x] if x else None)\n",
    "semcor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e19b5c-858a-4925-ac9e-178928c29ee5",
   "metadata": {},
   "source": [
    "## Vocab List\n",
    "\n",
    "We now have enough to start making a model. We can start by constructing a vocab list and document matrices etc so we can formulate the problem as a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3e56bfed-a83d-433e-be89-a8fe85a8b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = semcor_data.span.value_counts(ascending=True).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cffd6516-f09f-43a9-82b7-380bef220e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chockfull</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classified</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snapshots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Journalism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33591</th>\n",
       "      <td>not</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33592</th>\n",
       "      <td>are</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33593</th>\n",
       "      <td>be</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33594</th>\n",
       "      <td>was</td>\n",
       "      <td>4077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33595</th>\n",
       "      <td>is</td>\n",
       "      <td>5119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           span  count\n",
       "0      Fulton_County_Grand_Jury      1\n",
       "1                     chockfull      1\n",
       "2                    Classified      1\n",
       "3                     snapshots      1\n",
       "4                    Journalism      1\n",
       "...                         ...    ...\n",
       "33591                       not   1607\n",
       "33592                       are   1867\n",
       "33593                        be   2160\n",
       "33594                       was   4077\n",
       "33595                        is   5119\n",
       "\n",
       "[33596 rows x 2 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "cf30b197-1ec4-43ba-8aa1-104ca1ad3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_to_id = {span: id for id, span in enumerate(vocab_data.span)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6307d4-578d-4dfd-833c-217416dc4734",
   "metadata": {},
   "source": [
    "## Calculate document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1259fc4c-f1b4-4b70-b7b1-138000891736",
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_data['document_vector'] = semcor_data.sentence.apply(lambda x: [span_to_id.get(span) for span in x if span in span_to_id.keys()])\n",
    "semcor_data['span_id'] = semcor_data.span.apply(lambda x: span_to_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "28a04d07-27ed-44ec-b5f6-1d5249950a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>document_vector</th>\n",
       "      <th>span_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305.0</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>33590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288.0</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>31097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>30785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>29054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224711</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>stung</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[30966, 20906, 14741, 33594, 33542, 33473, 334...</td>\n",
       "      <td>20906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224712</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>was</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[30966, 20906, 14741, 33594, 33542, 33473, 334...</td>\n",
       "      <td>33594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224713</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>let</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[30966, 20906, 14741, 33594, 33542, 33473, 334...</td>\n",
       "      <td>33484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224714</th>\n",
       "      <td>[Her, reply, stung, me, ,, but, this, was, too...</td>\n",
       "      <td>Her reply stung me , but this was too importan...</td>\n",
       "      <td>{reply, let, Her, stung, important, hurt, diff...</td>\n",
       "      <td>make</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[30966, 20906, 14741, 33594, 33542, 33473, 334...</td>\n",
       "      <td>33582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224715</th>\n",
       "      <td>[``, I, can, n't, turn, the, studio, into, a, ...</td>\n",
       "      <td>`` I can n't turn the studio into a gambling_h...</td>\n",
       "      <td>{n't, I, said, gambling_hell, studio, saloon, ...</td>\n",
       "      <td>said</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[26952, 33105, 30701, 16705, 16705, 20543, 33590]</td>\n",
       "      <td>33590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224716 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "3       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "4       [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "...                                                   ...   \n",
       "224711  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224712  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224713  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224714  [Her, reply, stung, me, ,, but, this, was, too...   \n",
       "224715  [``, I, can, n't, turn, the, studio, into, a, ...   \n",
       "\n",
       "                                            sentence_text  \\\n",
       "0       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "3       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "4       The Fulton_County_Grand_Jury said Friday an in...   \n",
       "...                                                   ...   \n",
       "224711  Her reply stung me , but this was too importan...   \n",
       "224712  Her reply stung me , but this was too importan...   \n",
       "224713  Her reply stung me , but this was too importan...   \n",
       "224714  Her reply stung me , but this was too importan...   \n",
       "224715  `` I can n't turn the studio into a gambling_h...   \n",
       "\n",
       "                                       processed_sentence  \\\n",
       "0       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "1       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "2       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "3       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "4       {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "...                                                   ...   \n",
       "224711  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224712  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224713  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224714  {reply, let, Her, stung, important, hurt, diff...   \n",
       "224715  {n't, I, said, gambling_hell, studio, saloon, ...   \n",
       "\n",
       "                            span         sense_key  sense_id  \\\n",
       "0       Fulton_County_Grand_Jury   group%1:03:00::     305.0   \n",
       "1                           said              None       NaN   \n",
       "2                         Friday  friday%1:28:00::     288.0   \n",
       "3                  investigation              None       NaN   \n",
       "4                        Atlanta              None       NaN   \n",
       "...                          ...               ...       ...   \n",
       "224711                     stung              None       NaN   \n",
       "224712                       was              None       NaN   \n",
       "224713                       let              None       NaN   \n",
       "224714                      make              None       NaN   \n",
       "224715                      said              None       NaN   \n",
       "\n",
       "                                          document_vector  span_id  \n",
       "0       [0, 33590, 31097, 30785, 29054, 33534, 32761, ...        0  \n",
       "1       [0, 33590, 31097, 30785, 29054, 33534, 32761, ...    33590  \n",
       "2       [0, 33590, 31097, 30785, 29054, 33534, 32761, ...    31097  \n",
       "3       [0, 33590, 31097, 30785, 29054, 33534, 32761, ...    30785  \n",
       "4       [0, 33590, 31097, 30785, 29054, 33534, 32761, ...    29054  \n",
       "...                                                   ...      ...  \n",
       "224711  [30966, 20906, 14741, 33594, 33542, 33473, 334...    20906  \n",
       "224712  [30966, 20906, 14741, 33594, 33542, 33473, 334...    33594  \n",
       "224713  [30966, 20906, 14741, 33594, 33542, 33473, 334...    33484  \n",
       "224714  [30966, 20906, 14741, 33594, 33542, 33473, 334...    33582  \n",
       "224715  [26952, 33105, 30701, 16705, 16705, 20543, 33590]    33590  \n",
       "\n",
       "[224716 rows x 8 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d27376-6c7a-4a3f-b775-ee92dcc3c304",
   "metadata": {},
   "source": [
    "Now that we have the term-document vectors we can drop the rows where the sense keys are not in our target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "99e15902-9d08-4318-aa2e-78b9ea4feb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>sense_key</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>document_vector</th>\n",
       "      <th>span_id</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>group%1:03:00::</td>\n",
       "      <td>305</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Fulton_County_Grand_Jury, said, Friday, ...</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an in...</td>\n",
       "      <td>{Fulton_County_Grand_Jury, Atlanta, took_place...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>friday%1:28:00::</td>\n",
       "      <td>288</td>\n",
       "      <td>[0, 33590, 31097, 30785, 29054, 33534, 32761, ...</td>\n",
       "      <td>31097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, said, presentment, over-all, deserves...</td>\n",
       "      <td>jury</td>\n",
       "      <td>jury%1:14:00::</td>\n",
       "      <td>378</td>\n",
       "      <td>[31734, 32919, 33590, 31785, 32231, 33457, 149...</td>\n",
       "      <td>31734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, said, presentment, over-all, deserves...</td>\n",
       "      <td>term</td>\n",
       "      <td>term%1:28:00::</td>\n",
       "      <td>728</td>\n",
       "      <td>[31734, 32919, 33590, 31785, 32231, 33457, 149...</td>\n",
       "      <td>32231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, jury, further, said, in, term, end, pres...</td>\n",
       "      <td>The jury further said in term end presentments...</td>\n",
       "      <td>{praise, said, presentment, over-all, deserves...</td>\n",
       "      <td>end</td>\n",
       "      <td>end%1:28:00::</td>\n",
       "      <td>234</td>\n",
       "      <td>[31734, 32919, 33590, 31785, 32231, 33457, 149...</td>\n",
       "      <td>33457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14109</th>\n",
       "      <td>[Without, further, discussion, he, appeared, t...</td>\n",
       "      <td>Without further discussion he appeared the nex...</td>\n",
       "      <td>{stained, morning, proceeded, according_to, di...</td>\n",
       "      <td>morning</td>\n",
       "      <td>morning%1:28:00::</td>\n",
       "      <td>450</td>\n",
       "      <td>[32919, 32753, 33334, 32607, 33172, 16705, 287...</td>\n",
       "      <td>33172</td>\n",
       "      <td>8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14110</th>\n",
       "      <td>[Before, you, use, ', em, the, light, company,...</td>\n",
       "      <td>Before you use ' em the light company 's got t...</td>\n",
       "      <td>{extra, circuit, 's, company, fuse, light, Bef...</td>\n",
       "      <td>company</td>\n",
       "      <td>company%1:14:01::</td>\n",
       "      <td>125</td>\n",
       "      <td>[33544, 33411, 33153, 33534, 33506, 33391, 317...</td>\n",
       "      <td>33153</td>\n",
       "      <td>8351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14111</th>\n",
       "      <td>[He, oughta, be, able, to, build, a, new, hous...</td>\n",
       "      <td>He oughta be able to build a new house with al...</td>\n",
       "      <td>{able, build, He, house, oughta, new, contrapt...</td>\n",
       "      <td>house</td>\n",
       "      <td>house%1:06:00::</td>\n",
       "      <td>332</td>\n",
       "      <td>[33593, 33240, 32995, 16705, 33561, 33444, 335...</td>\n",
       "      <td>33444</td>\n",
       "      <td>8352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14112</th>\n",
       "      <td>[Mr._Crombie, watched, his, wife, with, an, an...</td>\n",
       "      <td>Mr._Crombie watched his wife with an anxious e...</td>\n",
       "      <td>{anxious, wife, watched, expression, Mr._Crombie}</td>\n",
       "      <td>wife</td>\n",
       "      <td>wife%1:18:00::</td>\n",
       "      <td>807</td>\n",
       "      <td>[29571, 33087, 33395, 19670, 32701]</td>\n",
       "      <td>33395</td>\n",
       "      <td>8353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14113</th>\n",
       "      <td>[``, I, 've, been, waiting, to, get, these, th...</td>\n",
       "      <td>`` I 've been waiting to get these things done...</td>\n",
       "      <td>{month, said, I, 've, done, thing, get, waiting}</td>\n",
       "      <td>months</td>\n",
       "      <td>month%1:28:01::</td>\n",
       "      <td>447</td>\n",
       "      <td>[8420, 33585, 33142, 33566, 33468, 33512, 3304...</td>\n",
       "      <td>33049</td>\n",
       "      <td>8354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14114 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "1      [The, Fulton_County_Grand_Jury, said, Friday, ...   \n",
       "2      [The, jury, further, said, in, term, end, pres...   \n",
       "3      [The, jury, further, said, in, term, end, pres...   \n",
       "4      [The, jury, further, said, in, term, end, pres...   \n",
       "...                                                  ...   \n",
       "14109  [Without, further, discussion, he, appeared, t...   \n",
       "14110  [Before, you, use, ', em, the, light, company,...   \n",
       "14111  [He, oughta, be, able, to, build, a, new, hous...   \n",
       "14112  [Mr._Crombie, watched, his, wife, with, an, an...   \n",
       "14113  [``, I, 've, been, waiting, to, get, these, th...   \n",
       "\n",
       "                                           sentence_text  \\\n",
       "0      The Fulton_County_Grand_Jury said Friday an in...   \n",
       "1      The Fulton_County_Grand_Jury said Friday an in...   \n",
       "2      The jury further said in term end presentments...   \n",
       "3      The jury further said in term end presentments...   \n",
       "4      The jury further said in term end presentments...   \n",
       "...                                                  ...   \n",
       "14109  Without further discussion he appeared the nex...   \n",
       "14110  Before you use ' em the light company 's got t...   \n",
       "14111  He oughta be able to build a new house with al...   \n",
       "14112  Mr._Crombie watched his wife with an anxious e...   \n",
       "14113  `` I 've been waiting to get these things done...   \n",
       "\n",
       "                                      processed_sentence  \\\n",
       "0      {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "1      {Fulton_County_Grand_Jury, Atlanta, took_place...   \n",
       "2      {praise, said, presentment, over-all, deserves...   \n",
       "3      {praise, said, presentment, over-all, deserves...   \n",
       "4      {praise, said, presentment, over-all, deserves...   \n",
       "...                                                  ...   \n",
       "14109  {stained, morning, proceeded, according_to, di...   \n",
       "14110  {extra, circuit, 's, company, fuse, light, Bef...   \n",
       "14111  {able, build, He, house, oughta, new, contrapt...   \n",
       "14112  {anxious, wife, watched, expression, Mr._Crombie}   \n",
       "14113   {month, said, I, 've, done, thing, get, waiting}   \n",
       "\n",
       "                           span          sense_key  sense_id  \\\n",
       "0      Fulton_County_Grand_Jury    group%1:03:00::       305   \n",
       "1                        Friday   friday%1:28:00::       288   \n",
       "2                          jury     jury%1:14:00::       378   \n",
       "3                          term     term%1:28:00::       728   \n",
       "4                           end      end%1:28:00::       234   \n",
       "...                         ...                ...       ...   \n",
       "14109                   morning  morning%1:28:00::       450   \n",
       "14110                   company  company%1:14:01::       125   \n",
       "14111                     house    house%1:06:00::       332   \n",
       "14112                      wife     wife%1:18:00::       807   \n",
       "14113                    months    month%1:28:01::       447   \n",
       "\n",
       "                                         document_vector  span_id  sentence_id  \n",
       "0      [0, 33590, 31097, 30785, 29054, 33534, 32761, ...        0            0  \n",
       "1      [0, 33590, 31097, 30785, 29054, 33534, 32761, ...    31097            0  \n",
       "2      [31734, 32919, 33590, 31785, 32231, 33457, 149...    31734            1  \n",
       "3      [31734, 32919, 33590, 31785, 32231, 33457, 149...    32231            1  \n",
       "4      [31734, 32919, 33590, 31785, 32231, 33457, 149...    33457            1  \n",
       "...                                                  ...      ...          ...  \n",
       "14109  [32919, 32753, 33334, 32607, 33172, 16705, 287...    33172         8350  \n",
       "14110  [33544, 33411, 33153, 33534, 33506, 33391, 317...    33153         8351  \n",
       "14111  [33593, 33240, 32995, 16705, 33561, 33444, 335...    33444         8352  \n",
       "14112                [29571, 33087, 33395, 19670, 32701]    33395         8353  \n",
       "14113  [8420, 33585, 33142, 33566, 33468, 33512, 3304...    33049         8354  \n",
       "\n",
       "[14114 rows x 9 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_data = semcor_data[~semcor_data.sense_key.isnull()]\n",
    "sentence_to_id = {v: k for k, v in enumerate(semcor_data.sentence_text.unique())}\n",
    "semcor_data['sentence_id'] = semcor_data.sentence_text.apply(lambda x: sentence_to_id[x])\n",
    "\n",
    "semcor_data['sense_id'] = semcor_data.loc[:, 'sense_id'].astype(np.int32)\n",
    "semcor_data = semcor_data.reset_index(drop=True)\n",
    "semcor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53600682-264e-4c6f-b2b0-6bbc735bde62",
   "metadata": {},
   "source": [
    "## Convert to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d19e6e10-9a58-48d5-b96e-705a07bd9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(semcor_data):\n",
    "    \n",
    "    # Flatten the Series to create a list of (doc_id, word_id) tuples\n",
    "    rows, cols = zip(*((id, token) for id, row in semcor_data.iterrows() for token in row.document_vector))\n",
    "    \n",
    "    # Create a sparse matrix\n",
    "    # The shape parameters (n_rows, n_cols) should match your data dimensions\n",
    "    n_rows = len(semcor_data)\n",
    "    n_cols = semcor_data.document_vector.apply(max).max() + 1  # assuming token IDs start from 0\n",
    "    data = [1] * len(rows)  # assuming a count of 1 for each occurrence\n",
    "    term_document_matrix = sparse.coo_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "    \n",
    "    # Convert to CSR format for efficient arithmetic and matrix-vector operations\n",
    "    return term_document_matrix.tocsr()\n",
    "\n",
    "term_document_matrix = create_term_document_matrix(semcor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "296723c2-e3d9-45a9-ad69-44a416da12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data - replace with your actual data\n",
    "X = term_document_matrix\n",
    "y = semcor_data.sense_id\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef7f0a-40ba-4575-880f-55fa749a61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb498ca5-ed2e-4ab2-be57-c4d93738f425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
