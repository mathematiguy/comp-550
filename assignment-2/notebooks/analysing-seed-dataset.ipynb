{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce17e99-30f2-4078-976b-7751f10735a6",
   "metadata": {},
   "source": [
    "# Analysing Seed Dataset\n",
    "\n",
    "In this notebook we'll use the seed dataset we generated to bootstrap a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3dddeb-0fc0-42c0-8b76-625c101946c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb434f9-d436-4a6f-b537-5b703925d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mila/c/caleb.moses/venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/mila/c/caleb.moses/venv/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mila/c/caleb.moses/venv/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mila/c/caleb.moses/venv/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/mila/c/caleb.moses/venv/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946612a4-946f-42d1-85e5-e0e4b755bd93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex._regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/nltk/__init__.py:138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/nltk/text.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist \u001b[38;5;28;01mas\u001b[39;00m CFD\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyConcatenation, tokenwrap\n\u001b[1;32m     32\u001b[0m ConcordanceLine \u001b[38;5;241m=\u001b[39m namedtuple(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcordanceLine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer, casual_tokenize\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdestructive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTKWordTokenizer\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegality_principle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LegalitySyllableTokenizer\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/nltk/tokenize/casual.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhtml\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m  \u001b[38;5;66;03m# https://github.com/nltk/nltk/issues/2409\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizerI\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# The following strings are components in the regular expression\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# that is used for tokenizing. It's important that phone_number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# This particular element is used in a couple ways, so we define it\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# with a name:\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/regex/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regex\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39m__all__\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/regex/regex.py:421\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Internals.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regex_core\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_regex_core\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_regex\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RLock \u001b[38;5;28;01mas\u001b[39;00m _RLock\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/regex/_regex_core.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_regex\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASCII\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBESTMATCH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENHANCEMATCH\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULLCASE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIGNORECASE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCALE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMULTILINE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOSIX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREVERSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOTALL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPLATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNICODE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERSION0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERSION1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERBOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScanner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegexFlag\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# The regex exception.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex._regex'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from build_seed_set import load_seed_dataset\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea74f3-693d-4228-a698-2a56f28723e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dataset = load_seed_dataset('../data/seed_set_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc911e-3f53-4ec7-bea1-0b68133fc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52090f-e29e-4865-8e9b-9a6deb4f473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7804aa-6b9b-4aa4-9808-f17830722d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_examples = seed_dataset[['word', 'synset_id', 'generated_examples', 'examples']].copy()\n",
    "seed_examples['text'] = seed_examples.apply(lambda x: x.examples + x.generated_examples, axis=1)\n",
    "seed_examples = seed_examples.loc[:, ['word', 'synset_id', 'text']]\n",
    "seed_examples = seed_examples.explode('text')\n",
    "seed_examples = seed_examples[seed_examples.apply(lambda x: x.word in x.text, axis=1)]\n",
    "seed_examples = seed_examples.reset_index(drop=True)\n",
    "seed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232180b-f67a-437c-98e1-561998afecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_examples.synset_id.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45950d-a0c7-46de-ae58-d3657817ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory:\n",
    "    def __init__(self, texts, labels, tokenizer, stop_words, C=1e12, class_weight='balanced', test_size=0.2):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.C = C\n",
    "        self.class_weight = class_weight\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def lemmatize_tokenize(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_tokens = [self.tokenizer.lemmatize(token) for token in tokens if not token in self.stop_words]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def train(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.texts, self.labels, test_size=self.test_size)\n",
    "\n",
    "        vectorizer = CountVectorizer(tokenizer=self.lemmatize_tokenize)\n",
    "        X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "        clf = MultiOutputRegressor(LogisticRegression(C=self.C, class_weight=self.class_weight))\n",
    "        clf.fit(X_train_counts, y_train)\n",
    "\n",
    "        return clf, vectorizer, X_train_counts, X_test, y_train, y_test\n",
    "\n",
    "    def evaluate(self, clf, vectorizer, X_train_counts, X_test, y_train, y_test):\n",
    "        train_predictions = clf.predict(X_train_counts)\n",
    "        preds = np.argmax(train_predictions, axis=1)\n",
    "        targets = np.argmax(y_train, axis=1)\n",
    "        train_accuracy = np.mean(preds == targets)\n",
    "\n",
    "        X_test_counts = vectorizer.transform(X_test)\n",
    "        test_predictions = clf.predict(X_test_counts)\n",
    "        test_preds = np.argmax(test_predictions, axis=1)\n",
    "        test_targets = np.argmax(y_test, axis=1)\n",
    "        test_accuracy = np.mean(test_preds == test_targets)\n",
    "\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "# Example usage\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "factory = ModelFactory(\n",
    "    texts=seed_examples.text,\n",
    "    labels=pd.get_dummies(seed_examples['synset_id']) * 1,\n",
    "    tokenizer=lemmatizer,\n",
    "    stop_words=stop_words\n",
    ")\n",
    "clf, vectorizer, X_train_counts, X_test, y_train, y_test = factory.train()\n",
    "train_accuracy, test_accuracy = factory.evaluate(clf, vectorizer, X_train_counts, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2e1d6-d09e-4679-b175-2dce9ce5626d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (whakamarama-reo-venv)",
   "language": "python",
   "name": "whakamarama-reo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
