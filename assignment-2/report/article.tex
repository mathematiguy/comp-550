%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Comp 550: Programming Assignment 2}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

%% \listoffigures % Print the list of figures

%% \listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%% \section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)

%% This paper explores the role of artificial intelligence for natural language in supporting communities affected by colonialism to reclaim and revitalise their languages. We order well known research questions in natural language processing for low research languages by their potential impact for language reclamation, and also explain the necessary language data required for these. Availability of language data varies widely by community and also data type such as written, audio-video or image scans. Fundamentally we aim to bridge a gap between industry, where much work in this area exists, and community where data is often available but not easily utilised due to funding and technical capability gaps.

%----------------------------------------------------------------------------------------
%	AUTHOR AFFILIATIONS
%----------------------------------------------------------------------------------------

\let\thefootnote\relax\footnotetext{* \textit{PhD Student, School of Computer Science, McGill University, Montreal, Canada}}

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

This report describes approaches to Word Sense Disambiguation, in particular applied to the SemEval 2013 Shared Task \#12.

%----------------------------------------------------------------------------------------
%	METHODS
%----------------------------------------------------------------------------------------

\section{Word Sense Disambiguation}

\subsection{Lesk's Algorithm}

For this project, I implemented Lesk's Algorithm using WordNet as the dictionary. I preprocessed each instance from SemEval into a bag of words where each token was lemmatized, stopwords were removed and words containing no alphanumeric characters (i.e. punctuation) were also removed. Doing this, I achieved an accuracy score on the dev set provided of 57.2\%, and a test set accuracy of 50.5\%.

I then computed dev and test set accuracy for both the most frequent synset baseline, as well as the baseline using the NLTK implementation of Lesk's Algorithm. The most frequent synset accuracy was 67.5\% on the dev set and 62.3\% on the test set, and the NLTK Lesk's Algorithm implementation achieved an accuracy of 34.0\% on the dev set and 34.1\% on the test set.

\subsection{Bootstrap Model}

\subsubsection{Semcor Dataset}
Next, I built a bootstrap model for the Word Sense Disambiguation problem. In order to generate a seed set for bootstrapping, I started by using the semcor dataset, which is a synset tagged subset of the Brown corpus, available in NLTK. I used the correspondence between the lemma sense keys and synset numbers to select out the examples in SemCor which are tagged with lemma sense keys that exist in either the dev or test set of SemEval. I then created term-document matrices applying the same preprocessing methods as I did for Lesk's Algorithm and I used those to fit a logistic regression for classifying the lemma sense key for each example in SemCor.

However, the result I got from this model was very poor (less than 1\% accuracy) and so I abandoned the idea early. In later experiments, I realised that the default hyperparameters I used for the logistic regression were a poor choice, and also that it was possible to address the class imbalance when instantiating the model as well. When I went back and applied these methods, the test set accuracy of the model rose to 33.7\%.

When SemCor initially failed to perform well, I convinced myself that the dataset was a poor choice for the problem and considered other options. At the time I realised that the dev set examples from SemEval were mostly on matters of geo-politics and climate change and I thought the scope of the texts might be too narrow with also relatively few (around 140) examples.

\subsubsection{Bootstrap model}

When I gave up on SemCor I chose to use a Large Language Model (Llama-2-7B-Chat from Meta AI Research) in order to generate a seed set for bootstrapping. I did this by using wordnet to extract definitions and examples for each of the synsets covered in the dev set from SemEval. I then used those definitions to engineer prompts that asked Llama 2 to generate more examples in the same vein.

In order to have enough train and test examples for each of the synsets under consideration, I generated examples until there were at least 10 examples of each synset. Then, I used the seed set I generated to train a multiple-output logistic regression model for classifying each of the synsets under consideration. It was at this point that I noticed that I could improve the model significantly by turning off regularisation and turning on class balancing. The Logistic Regression model achieved a test set accuracy of 77.1\%.

However, working with Llama 2 was a complex task that turned out to be time consuming and as a result I was not able to complete the bootstrap model as intended.

\subsection{Improvements}

When I wrote this report I was inspired to go back to the SemCor model and check what happens if I fix the hyperparameters. Now that I see that was a big part of the problem, in hindsight I would have proceeded with bootstrapping for the SemCor model since there appeared to be a lot of room for improvement. I am also eager to see what would happen if I bootstrapped the Llama 2 seed set as well. My intention was to use the Reuter's dataset in NLTK, since I figured that would be a better thematic match to SemEval, since it's a news corpus.

Generating a seed set turned out to be labour intensive, but my own perfectionism is largely to blame since I became preoccupied with making a perfect seed set rather than working with something small and hand-crafted. I also wanted to cover as many synsets as possible, when I could've labelled fewer by hand.

Aside from that, in the project I only really worked with logistic regression since I consider it a good baseline. Of course, I intended on using other models and in particular I would have liked to try character-level embeddings in a second model since it would make it easier to transfer models across domains due to fewer out of vocab word problems, which was another frustration of working on this problem.

\subsection{Conclusion}

Overall I found this project challenging but interesting, and bootstrapping in particular turned out to be a tricky problem with a lot of things to consider. NLTK also turned out to be a lot more powerful than I've given it credit for. I have used it many times but mostly for basic tasks like tokenization and pre-processing, never for lexical semantics.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% \renewcommand{\refname}{\spacedlowsmallcaps{References}} % For modifying the bibliography heading

%% \bibliographystyle{unsrt}

%% \bibliography{sample.bib} % The file containing the bibliography

%----------------------------------------------------------------------------------------

\end{document}
